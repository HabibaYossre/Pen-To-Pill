{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11188431,"sourceType":"datasetVersion","datasetId":6984543},{"sourceId":11188441,"sourceType":"datasetVersion","datasetId":6984552}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TrOCR Fine-Tuning for Prescription Words (English + Arabic)\n\n**Dataset Structure:**\n- Excel file with columns: `image_name` | `word_label`\n- Folder containing all cropped word images","metadata":{}},{"cell_type":"code","source":"!pip install transformers torchvision pandas openpyxl pillow accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:01:50.331017Z","iopub.execute_input":"2025-03-27T22:01:50.331337Z","iopub.status.idle":"2025-03-27T22:01:55.069995Z","shell.execute_reply.started":"2025-03-27T22:01:50.331301Z","shell.execute_reply":"2025-03-27T22:01:55.069101Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom transformers import (\n    TrOCRProcessor,\n    VisionEncoderDecoderModel,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:03:44.529611Z","iopub.execute_input":"2025-03-27T22:03:44.529960Z","iopub.status.idle":"2025-03-27T22:04:07.096420Z","shell.execute_reply.started":"2025-03-27T22:03:44.529935Z","shell.execute_reply":"2025-03-27T22:04:07.095538Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 1. Load and Prepare Dataset","metadata":{}},{"cell_type":"code","source":"# Load your Excel file\ndf = pd.read_excel(\"/kaggle/input/excel-file/mohammed file.xlsx\")  # Change to your file path\nprint(f\"Loaded {len(df)} samples\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:04:07.097598Z","iopub.execute_input":"2025-03-27T22:04:07.098109Z","iopub.status.idle":"2025-03-27T22:04:07.590705Z","shell.execute_reply.started":"2025-03-27T22:04:07.098080Z","shell.execute_reply":"2025-03-27T22:04:07.589983Z"}},"outputs":[{"name":"stdout","text":"Loaded 292 samples\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                       Cropped Image Name          word\n0  Dental_prescription_605 (1)_crop_0.jpg        Ø§Ù„Ù„Ø²ÙˆÙ…\n1  Dental_prescription_605 (1)_crop_1.jpg   Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„\n2  Dental_prescription_605 (1)_crop_2.jpg  Ù…ÙŠØªØ±ÙˆÙ†ÙŠØ¯Ø§Ø²ÙˆÙ„\n3  Dental_prescription_605 (1)_crop_3.jpg        Ø§Ù„ØºØ°Ø§Ø¡\n4  Dental_prescription_605 (1)_crop_4.jpg           Ø¹Ù†Ø¯","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cropped Image Name</th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dental_prescription_605 (1)_crop_0.jpg</td>\n      <td>Ø§Ù„Ù„Ø²ÙˆÙ…</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dental_prescription_605 (1)_crop_1.jpg</td>\n      <td>Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dental_prescription_605 (1)_crop_2.jpg</td>\n      <td>Ù…ÙŠØªØ±ÙˆÙ†ÙŠØ¯Ø§Ø²ÙˆÙ„</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dental_prescription_605 (1)_crop_3.jpg</td>\n      <td>Ø§Ù„ØºØ°Ø§Ø¡</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dental_prescription_605 (1)_crop_4.jpg</td>\n      <td>Ø¹Ù†Ø¯</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Configuration\nIMAGE_FOLDER = \"/kaggle/input/cropped-images\"  # Change this\nTEST_SIZE = 0.1  # 10% for validation\nBATCH_SIZE = 8\nEPOCHS = 10\nMODEL_NAME = \"microsoft/trocr-base-handwritten\"  # or \"microsoft/trocr-base-stage1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:04:07.592275Z","iopub.execute_input":"2025-03-27T22:04:07.592697Z","iopub.status.idle":"2025-03-27T22:04:07.596736Z","shell.execute_reply.started":"2025-03-27T22:04:07.592673Z","shell.execute_reply":"2025-03-27T22:04:07.595661Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create PyTorch Dataset\nclass PrescriptionDataset(Dataset):\n    def __init__(self, df, processor, image_folder):\n        self.df = df\n        self.processor = processor\n        self.image_folder = image_folder\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get image name and label\n        img_name = self.df.iloc[idx]['Cropped Image Name']\n        text_label = str(self.df.iloc[idx]['word'])\n        \n        # Load image\n        image_path = os.path.join(self.image_folder, img_name)\n        image = Image.open(image_path).convert('RGB')\n\n        # Process image and text\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n        labels = self.processor.tokenizer(\n            text_label,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=64,\n            truncation=True\n        ).input_ids.squeeze()\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": labels\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:04:07.598073Z","iopub.execute_input":"2025-03-27T22:04:07.598450Z","iopub.status.idle":"2025-03-27T22:04:07.612646Z","shell.execute_reply.started":"2025-03-27T22:04:07.598406Z","shell.execute_reply":"2025-03-27T22:04:07.611755Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Initialize processor\nprocessor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n\n# Split data\ntrain_df = df.sample(frac=1-TEST_SIZE, random_state=42)\ntest_df = df.drop(train_df.index)\n\n# Create datasets\ntrain_dataset = PrescriptionDataset(train_df, processor, IMAGE_FOLDER)\neval_dataset = PrescriptionDataset(test_df, processor, IMAGE_FOLDER)\n\nprint(f\"Train samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:04:07.613444Z","iopub.execute_input":"2025-03-27T22:04:07.613658Z","iopub.status.idle":"2025-03-27T22:04:10.294538Z","shell.execute_reply.started":"2025-03-27T22:04:07.613640Z","shell.execute_reply":"2025-03-27T22:04:10.293758Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b961a577b944dd595f1d3a2d368181c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a08141ed04e44da95a1d5449b7fec39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1953e70b60ab4c7c98d02a88ed90c832"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac5b1fd72fd480085a951a59223dd83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b883eaee2f64fb7b212441201e7bee2"}},"metadata":{}},{"name":"stdout","text":"Train samples: 263, Eval samples: 29\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Initialize model","metadata":{}},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:04:10.295283Z","iopub.execute_input":"2025-03-27T22:04:10.295500Z","iopub.status.idle":"2025-03-27T22:04:22.148385Z","shell.execute_reply.started":"2025-03-27T22:04:10.295480Z","shell.execute_reply":"2025-03-27T22:04:22.147159Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d7c18e910f46b29098caf5476cfb14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba99e6f370a44dabd9b2f5a3da2acf4"}},"metadata":{}},{"name":"stderr","text":"Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n  \"attention_probs_dropout_prob\": 0.0,\n  \"encoder_stride\": 16,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 768,\n  \"image_size\": 384,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"model_type\": \"vit\",\n  \"num_attention_heads\": 12,\n  \"num_channels\": 3,\n  \"num_hidden_layers\": 12,\n  \"patch_size\": 16,\n  \"qkv_bias\": false,\n  \"transformers_version\": \"4.47.0\"\n}\n\nConfig of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"gelu\",\n  \"add_cross_attention\": true,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": 0.0,\n  \"cross_attention_hidden_size\": 768,\n  \"d_model\": 1024,\n  \"decoder_attention_heads\": 16,\n  \"decoder_ffn_dim\": 4096,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 12,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"layernorm_embedding\": true,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"trocr\",\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"transformers_version\": \"4.47.0\",\n  \"use_cache\": false,\n  \"use_learned_position_embeddings\": true,\n  \"vocab_size\": 50265\n}\n\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab78cc89e9424278851d46f0d8329cb2"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"./\",  # Use root directory to avoid nested folders\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    evaluation_strategy=\"steps\",  # More frequent, smaller evaluations\n    eval_steps=200,  # Evaluate every 200 steps\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    learning_rate=4e-5,\n    num_train_epochs=EPOCHS,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    fp16=True if torch.cuda.is_available() else False,\n    report_to=\"none\",\n    # Disable all saving to conserve space\n    save_strategy=\"no\",\n    save_total_limit=0,\n    load_best_model_at_end=False,\n    # Memory/performance optimizations\n    gradient_accumulation_steps=2,\n    fp16_full_eval=True,\n    generation_max_length=64,\n    generation_num_beams=1,\n    # Kaggle-specific optimizations\n    dataloader_pin_memory=False,  # Reduces memory usage\n    dataloader_num_workers=2,  # Optimal for Kaggle\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:15:38.443193Z","iopub.execute_input":"2025-03-27T22:15:38.443519Z","iopub.status.idle":"2025-03-27T22:15:38.472808Z","shell.execute_reply.started":"2025-03-27T22:15:38.443495Z","shell.execute_reply":"2025-03-27T22:15:38.472003Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    # Clip predictions to valid token ID range\n    pred_ids = np.clip(pred_ids, 0, len(processor.tokenizer) - 1)\n    \n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n\n    # Calculate character error rate (CER)\n    cer = 0\n    for pred, label in zip(pred_str, label_str):\n        # Simple CER calculation\n        cer += sum(1 for a, b in zip(pred, label) if a != b) / max(len(pred), len(label))\n    cer /= len(pred_str)\n\n    return {\"cer\": cer}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:15:40.265705Z","iopub.execute_input":"2025-03-27T22:15:40.266008Z","iopub.status.idle":"2025-03-27T22:15:40.271430Z","shell.execute_reply.started":"2025-03-27T22:15:40.265984Z","shell.execute_reply":"2025-03-27T22:15:40.270451Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Create trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Run training\ntrain_result = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:15:41.519389Z","iopub.execute_input":"2025-03-27T22:15:41.519713Z","iopub.status.idle":"2025-03-27T22:19:34.101292Z","shell.execute_reply.started":"2025-03-27T22:15:41.519686Z","shell.execute_reply":"2025-03-27T22:19:34.100122Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [160/160 03:50, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nn = 28\n# 2. Load test image\nfor i in range(n):\n    test_image_path = os.path.join(IMAGE_FOLDER, test_df.iloc[i]['Cropped Image Name'])\n    test_image = Image.open(test_image_path).convert('RGB')\n    # 3. Preprocess with device awareness\n    with torch.no_grad():\n        # Move inputs to same device as model\n        pixel_values = processor(test_image, return_tensors=\"pt\").pixel_values.to(device)\n        # Generate predictions\n        generated_ids = model.generate(pixel_values)\n        # Decode results\n        predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    # 4. Display results\n    print(\"\\n=== Prediction Test ===\")\n    print(f\"Image: {test_df.iloc[i]['Cropped Image Name']}\")\n    print(f\"Predicted: {predicted_text}\")\n    print(f\"Actual: {test_df.iloc[i]['word']}\")\n    # print(f\"Match: {predicted_text == test_df.iloc[0]['word']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T22:28:34.830327Z","iopub.execute_input":"2025-03-27T22:28:34.830613Z","iopub.status.idle":"2025-03-27T22:28:39.384498Z","shell.execute_reply.started":"2025-03-27T22:28:34.830591Z","shell.execute_reply":"2025-03-27T22:28:39.383781Z"}},"outputs":[{"name":"stdout","text":"\n=== Prediction Test ===\nImage: Dental_prescription_619 (1)_crop_11.jpg\nPredicted: 8\nActual: 8\n\n=== Prediction Test ===\nImage: Dental_prescription_607 (1)_crop_0.jpg\nPredicted: ÙƒÙ„\nActual: ÙƒÙ„\n\n=== Prediction Test ===\nImage: Dental_prescription_482_crop_4.jpg\nPredicted: 6\nActual: 6\n\n=== Prediction Test ===\nImage: Dental_prescription_543_crop_1.jpg\nPredicted: Ø§Ù„Ù‚Ø·Ø§Ø±\nActual: Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„\n\n=== Prediction Test ===\nImage: Dental_prescription_543_crop_5.jpg\nPredicted: Ø³Ø§Ø¹Ø§Øª\nActual: Ø§Ù„ÙØ·Ø§Ø±\n\n=== Prediction Test ===\nImage: Dental_prescription_543_crop_9.jpg\nPredicted: Ø§Ù„ÙØ·Ø§Ø±\nActual: Ø§Ù„ÙØ·Ø§Ø±\n\n=== Prediction Test ===\nImage: Dental_prescription_566_crop_5.jpg\nPredicted: Ù‚Ø¨Ù„\nActual: Ù‚Ø¨Ù„\n\n=== Prediction Test ===\nImage: Dental_prescription_557_crop_10.jpg\nPredicted: Ø³ÙŠØ¨Ø±ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†\nActual: Ø³ÙŠØ¨Ø±ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†\n\n=== Prediction Test ===\nImage: Dental_prescription_619_crop_9.jpg\nPredicted: 6\nActual: 6\n\n=== Prediction Test ===\nImage: Dental_prescription_621_crop_0.jpg\nPredicted: Ø§Ù„ÙØ·Ø§Ø±\nActual: Ø§Ù„ÙØ·Ø§Ø±\n\n=== Prediction Test ===\nImage: Dental_prescription_621_crop_4.jpg\nPredicted: Ø§Ù„ØºØ°Ø§Ø¡\nActual: Ø§Ù„ØºØ°Ø§Ø¡\n\n=== Prediction Test ===\nImage: Dental_prescription_620 (1)_crop_1.jpg\nPredicted: Ø§Ù„Ù†ÙˆÙ…\nActual: Ø§Ù„Ù†ÙˆÙ…\n\n=== Prediction Test ===\nImage: Dental_prescription_620 (1)_crop_10.jpg\nPredicted: Ø¨Ø¹Ø¯\nActual: Ø¨Ø¹Ø¯\n\n=== Prediction Test ===\nImage: Dental_prescription_539_crop_9.jpg\nPredicted: ÙƒÙ„\nActual: ÙƒÙ„\n\n=== Prediction Test ===\nImage: Dental_prescription_539_crop_11.jpg\nPredicted: 12\nActual: 12\n\n=== Prediction Test ===\nImage: Dental_prescription_611 (1)_crop_0.jpg\nPredicted: twice\nActual: twice\n\n=== Prediction Test ===\nImage: Dental_prescription_611 (1)_crop_9.jpg\nPredicted: as\nActual: as\n\n=== Prediction Test ===\nImage: Dental_prescription_603_crop_1.jpg\nPredicted: Ø§Ù„ØºØ°Ø§Ø¡\nActual: Ø§Ù„ØºØ°Ø§Ø¡\n\n=== Prediction Test ===\nImage: Dental_prescription_611_crop_8.jpg\nPredicted: as\nActual: as\n\n=== Prediction Test ===\nImage: Dental_prescription_611_crop_9.jpg\nPredicted: as\nActual: as\n\n=== Prediction Test ===\nImage: Dental_prescription_611_crop_10.jpg\nPredicted: if\nActual: if\n\n=== Prediction Test ===\nImage: Dental_prescription_611_crop_12.jpg\nPredicted: needed\nActual: needed\n\n=== Prediction Test ===\nImage: Dental_prescription_617 (1)_crop_0.jpg\nPredicted: ÙƒÙ„\nActual: ÙƒÙ„\n\n=== Prediction Test ===\nImage: Dental_prescription_542_crop_3.jpg\nPredicted: Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„\nActual: Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„\n\n=== Prediction Test ===\nImage: Dental_prescription_542_crop_11.jpg\nPredicted: 8\nActual: 8\n\n=== Prediction Test ===\nImage: Dental_prescription_609_crop_1.jpg\nPredicted: Ø³Ø§Ø¹Ø§Øª\nActual: Ø³Ø§Ø¹Ø§Øª\n\n=== Prediction Test ===\nImage: Dental_prescription_609_crop_6.jpg\nPredicted: Ø§Ù…Ø§ÙŠØ³ÙŠÙ†\nActual: Ø§Ù…Ø§ÙŠØ³ÙŠÙ†\n\n=== Prediction Test ===\nImage: Dental_prescription_602 (1)_crop_2.jpg\nPredicted: Ø³Ø§Ø¹Ø§Øª\nActual: Ø³Ø§Ø¹Ø§Øª\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}