# -*- coding: utf-8 -*-

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
mohammed237_machathon6_phase1_images_path = kagglehub.dataset_download('mohammed237/machathon6-phase1-images')
mohammed237_merge_file_path = kagglehub.dataset_download('mohammed237/merge-file')
mohammed237_yolo_weights_word_detection_pytorch_default_1_path = kagglehub.model_download('mohammed237/yolo-weights-word-detection/PyTorch/default/1')
mohammed237_trocr_finetune_weights_stp_pytorch_default_1_path = kagglehub.model_download('mohammed237/trocr_finetune_weights_stp/PyTorch/default/1')

print('Data source import complete.')

"""# **Import Libraries**"""

!pip install jiwer
!pip install ultralytics
!pip install fuzzywuzzy

import os
import cv2
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as transforms
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from google.colab import drive
import gdown
from ultralytics import YOLO
import re
from rapidfuzz import fuzz, process
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
import cv2

"""# **Load YOLO Wieghts**"""

# drive.mount('/content/drive')
# file_id = "1EJRP-VYKdhKxUUcU0U8P6kSyo03SqK3B"
# output = "/content/best.pt"  # Save the file in Colab's working directory
# # Download the file from Google Drive
# gdown.download(f"https://drive.google.com/uc?id={file_id}", output, quiet=False)
#yoloV8 wieghts
model = YOLO("/kaggle/input/yolo-weights-word-detection/pytorch/default/1/worddetection.pt")  # Path to your downloaded weights

"""# **Declare DB**"""

medicine_list=['Acetazolamide', 'Acetylcysteine', 'Albendazole', 'Amantadine', 'Amoxicillin', 'Artificial Tears', 'Atropine', 'Azithromycin', 'Bimatoprost', 'Botox', 'Brimonidine', 'Calcium Carbonate', 'Carbachol', 'Carbamazepine', 'Carbocisteine', 'Cefdinir', 'Cefixime', 'Cefuroxime', 'Cetirizine', 'Chloramphenicol', 'Chlorhexidine', 'Ciprofloxacin', 'Clindamycin', 'Collagen', 'Cyclosporine', 'Dexamethasone', 'Dextromethorphan', 'Diclofenac', 'Domperidone', 'Donepezil', 'Dorzolamide', 'Doxycycline', 'Erythromycin', 'Fluorometholone', 'Gabapentin', 'Gatifloxacin', 'Glycolic Acid', 'Guaifenesin', 'Hyaluronic Acid', 'Hydrocortisone', 'Hydroquinone', 'Hydroxyzine', 'Ibuprofen', 'Iron Supplements', 'Ketorolac', 'Lactic Acid', 'Latanoprost', 'Levetiracetam', 'Levofloxacin', 'Lidocaine', 'Lifitegrast', 'Loratadine', 'Mannitol', 'Mebendazole', 'Memantine', 'Metoclopramide', 'Metronidazole', 'Montelukast', 'Moxifloxacin', 'Multivitamins', 'Mupirocin', 'Natamycin', 'Nepafenac', 'Niacinamide', 'Nystatin', 'Ofloxacin', 'Omeprazole', 'Ondansetron', 'Oral Rehydration Salts', 'Paracetamol', 'Peptides', 'Pilocarpine', 'Pramipexole', 'Prednisolone', 'Prednisolone Acetate', 'Prednisolone Drops', 'Pregabalin', 'Probiotics', 'Ranitidine', 'Retinol', 'Rivastigmine', 'Ropinirole', 'Salbutamol', 'Salicylic Acid', 'Sodium Hyaluronate', 'Timolol', 'Tobramycin', 'Topiramate', 'Travoprost', 'Tretinoin', 'Tropicamide', 'Valproate', 'Vitamin C Serum', 'Vitamin D', 'Voriconazole', 'Zinc Sulfate', 'Ø£ØªØ±ÙˆØ¨ÙŠÙ†', 'Ø£Ø²ÙŠØ«Ø±ÙˆÙ…ÙŠØ³ÙŠÙ†', 'Ø£Ø³ÙŠØªØ§Øª Ø§Ù„Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†', 'Ø£Ø³ÙŠØªØ§Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯', 'Ø£Ø³ÙŠØªÙŠÙ„ Ø³ÙŠØ³ØªÙŠÙ†', 'Ø£Ù„Ø¨ÙŠÙ†Ø¯Ø§Ø²ÙˆÙ„', 'Ø£Ù…Ø§Ù†ØªØ§Ø¯ÙŠÙ†', 'Ø£Ù…Ù„Ø§Ø­ Ø§Ù„Ø¥Ù…Ø§Ù‡Ø© Ø§Ù„ÙÙ…ÙˆÙŠØ©', 'Ø£Ù…ÙˆÙƒØ³ÙŠØ³ÙŠÙ„ÙŠÙ†', 'Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ø£ÙˆÙ…ÙŠØ¨Ø±Ø§Ø²ÙˆÙ„', 'Ø£ÙˆÙ†Ø¯Ø§Ù†Ø³ÙŠØªØ±ÙˆÙ†', 'Ø¥Ø±ÙŠØ«Ø±ÙˆÙ…Ø§ÙŠØ³ÙŠÙ†', 'Ø¥ÙŠØ¨ÙˆØ¨Ø±ÙˆÙÙŠÙ†', 'Ø§Ù„Ø¨Ø¨ØªÙŠØ¯Ø§Øª', 'Ø§Ù„Ø¨Ø±ÙˆØ¨ÙŠÙˆØªÙŠÙƒ', 'Ø§Ù„Ø¨ÙˆØªÙˆÙƒØ³', 'Ø§Ù„ÙÙŠØªØ§Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©', 'Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„', 'Ø¨Ø±Ø§Ù…ÙŠØ¨ÙŠÙƒØ³ÙˆÙ„', 'Ø¨Ø±ÙŠØ¬Ø§Ø¨Ø§Ù„ÙŠÙ†', 'Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†', 'Ø¨Ø±ÙŠÙ…ÙˆÙ†ÙŠØ¯ÙŠÙ†', 'Ø¨ÙŠÙ„ÙˆÙƒØ§Ø±Ø¨ÙŠÙ†', 'Ø¨ÙŠÙ…Ø§ØªÙˆØ¨Ø±ÙˆØ³Øª', 'ØªØ±Ø§ÙÙˆØ¨Ø±ÙˆØ³Øª', 'ØªØ±ÙˆØ¨ÙŠÙƒØ§Ù…ÙŠØ¯', 'ØªØ±ÙŠØªÙŠÙ†ÙˆÙŠÙ†', 'ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ†', 'ØªÙˆØ¨ÙŠØ±Ø§Ù…ÙŠØª', 'ØªÙŠÙ…ÙˆÙ„ÙˆÙ„', 'Ø¬Ø§Ø¨Ø§Ø¨Ù†ØªÙŠÙ†', 'Ø¬Ø§ØªÙŠÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ø­Ù…Ø¶ Ø§Ù„Ø¬Ù„ÙŠÙƒÙˆÙ„ÙŠÙƒ', 'Ø­Ù…Ø¶ Ø§Ù„Ø³Ø§Ù„ÙŠØ³ÙŠÙ„ÙŠÙƒ', 'Ø­Ù…Ø¶ Ø§Ù„Ù„Ø§ÙƒØªÙŠÙƒ', 'Ø­Ù…Ø¶ Ø§Ù„Ù‡ÙŠØ§Ù„ÙˆØ±ÙˆÙ†ÙŠÙƒ', 'Ø¯Ù…ÙˆØ¹ ØµÙ†Ø§Ø¹ÙŠØ©', 'Ø¯ÙˆØ±Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯', 'Ø¯ÙˆÙƒØ³ÙŠØ³ÙŠÙƒÙ„ÙŠÙ†', 'Ø¯ÙˆÙ…Ø¨ÙŠØ±ÙŠØ¯ÙˆÙ†', 'Ø¯ÙˆÙ†ÙŠØ¨ÙŠØ²ÙŠÙ„', 'Ø¯ÙŠÙƒØ³Ø§Ù…ÙŠØ«Ø§Ø²ÙˆÙ†', 'Ø¯ÙŠÙƒØ³ØªØ±ÙˆÙ…ÙŠØ«ÙˆØ±ÙØ§Ù†', 'Ø¯ÙŠÙƒÙ„ÙˆÙÙŠÙ†Ø§Ùƒ', 'Ø±Ø§Ù†ÙŠØªÙŠØ¯ÙŠÙ†', 'Ø±ÙˆØ¨ÙŠÙ†ÙŠØ±ÙˆÙ„', 'Ø±ÙŠØªÙŠÙ†ÙˆÙ„', 'Ø±ÙŠÙØ§Ø³ØªØ¬Ù…ÙŠÙ†', 'Ø³Ø§Ù„Ø¨ÙŠÙˆØªØ§Ù…ÙˆÙ„', 'Ø³ÙŠØ¨Ø±ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ø³ÙŠØªØ±ÙŠØ²ÙŠÙ†', 'Ø³ÙŠÙØ¯ÙŠÙ†ÙŠØ±', 'Ø³ÙŠÙÙˆØ±ÙŠÙˆÙƒØ³ÙŠÙ…', 'Ø³ÙŠÙÙŠÙƒØ³ÙŠÙ…', 'Ø³ÙŠÙƒÙ„ÙˆØ³Ø¨ÙˆØ±ÙŠÙ†', 'ØºÙˆØ§ÙŠÙÙŠÙ†ÙŠØ³ÙŠÙ†', 'ÙØ§Ù„Ø¨Ø±ÙˆØ§Øª', 'ÙÙ„ÙˆØ±ÙˆÙ…ÙŠØ«ÙˆÙ„ÙˆÙ†', 'ÙÙˆØ±ÙŠÙƒÙˆÙ†Ø§Ø²ÙˆÙ„', 'ÙÙŠØªØ§Ù…ÙŠÙ† Ø¯', 'Ù‚Ø·Ø±Ø§Øª Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†', 'ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„', 'ÙƒØ§Ø±Ø¨Ø§Ù…Ø§Ø²ÙŠØ¨ÙŠÙ†', 'ÙƒØ§Ø±Ø¨ÙˆØ³ÙŠØ³ØªÙŠÙ†', 'ÙƒØ¨Ø±ÙŠØªØ§Øª Ø§Ù„Ø²Ù†Ùƒ', 'ÙƒØ±Ø¨ÙˆÙ†Ø§Øª Ø§Ù„ÙƒØ§Ù„Ø³ÙŠÙˆÙ…', 'ÙƒÙ„ÙˆØ±Ø§Ù…ÙÙŠÙ†ÙŠÙƒÙˆÙ„', 'ÙƒÙ„ÙˆØ±Ù‡ÙŠÙƒØ³ÙŠØ¯ÙŠÙ†', 'ÙƒÙ„ÙŠÙ†Ø¯Ø§Ù…Ø§ÙŠØ³ÙŠÙ†', 'ÙƒÙˆÙ„Ø§Ø¬ÙŠÙ†', 'ÙƒÙŠØªÙˆØ±ÙˆÙ„Ø§Ùƒ', 'Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª', 'Ù„ÙˆØ±Ø§ØªØ§Ø¯ÙŠÙ†', 'Ù„ÙŠØ¯ÙˆÙƒØ§ÙŠÙŠÙ†', 'Ù„ÙŠÙÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ù„ÙŠÙÙŠØªÙŠØ¬Ø±Ø§Ø³Øª', 'Ù„ÙŠÙÙŠØªÙŠØ±Ø§Ø³ÙŠØªØ§Ù…', 'Ù…Ø§Ù†ÙŠØªÙˆÙ„', 'Ù…ØµÙ„ ÙÙŠØªØ§Ù…ÙŠÙ† Ø³ÙŠ', 'Ù…ÙƒÙ…Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ¯', 'Ù…ÙˆØ¨ÙŠØ±ÙˆØ³ÙŠÙ†', 'Ù…ÙˆÙƒØ³ÙŠÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ù…ÙˆÙ†ØªÙŠÙ„ÙˆÙƒØ§Ø³Øª', 'Ù…ÙŠØ¨ÙŠÙ†Ø¯Ø§Ø²ÙˆÙ„', 'Ù…ÙŠØªØ±ÙˆÙ†ÙŠØ¯Ø§Ø²ÙˆÙ„', 'Ù…ÙŠØªÙˆÙƒÙ„ÙˆØ¨Ø±Ø§Ù…ÙŠØ¯', 'Ù…ÙŠÙ…Ø§Ù†ØªÙŠÙ†', 'Ù†Ø§ØªØ§Ù…ÙŠØ³ÙŠÙ†', 'Ù†ÙŠØ§Ø³ÙŠÙ†Ø§Ù…ÙŠØ¯', 'Ù†ÙŠØ¨Ø§ÙÙŠÙ†Ø§Ùƒ', 'Ù†ÙŠØ³ØªØ§ØªÙŠÙ†', 'Ù‡ÙŠØ§Ù„ÙˆØ±ÙˆÙ†Ø§Øª Ø§Ù„ØµÙˆØ¯ÙŠÙˆÙ…', 'Ù‡ÙŠØ¯Ø±ÙˆÙƒØ³ÙŠØ²ÙŠÙ†', 'Ù‡ÙŠØ¯Ø±ÙˆÙƒÙˆØ±ØªÙŠØ²ÙˆÙ†', 'Ù‡ÙŠØ¯Ø±ÙˆÙƒÙŠÙ†ÙˆÙ†']

ArabicWords=['Ù¨', 'Ø¯Ù‚Ø§Ø¦Ù‚', 'Ù¡Ù¢', 'Ù‚Ø¨Ù„', 'Ø§Ù„Ø£ÙƒÙ„', 'Ø£Ø³Ø¨ÙˆØ¹', 'Ù¥', 'Ø§Ù„Ù†ÙˆÙ…', 'Ù„ÙŠÙ„Ø§', 'Ø³Ø§Ø¹Ø§Øª', 'Ù„Ø£ÙˆÙ„', 'ÙŠÙˆÙ…', 'Ù…Ø±Ø©', 'Ø§Ù„ÙŠÙ‚Ø¸Ø©', 'Ù¤', 'ÙƒÙ„', 'Ù§', 'Ø¯Ù‚ÙŠÙ‚Ø©', 'Ø£Ø«Ù†Ø§Ø¡', 'Ù£', 'Ù¦', 'ÙŠÙˆÙ…ÙŠÙ†', 'Ø§Ù„ÙØ·Ø§Ø±', 'Ù…Ø±ØªÙŠÙ†', 'Ù¡Ù¤', 'ÙˆØ§Ø­Ø¯Ø©', 'Ø£ÙŠØ§Ù…', 'Ø§Ù„ØºØ¯Ø§Ø¡', 'Ø§Ù„Ù„Ø²ÙˆÙ…', 'Ø³Ø§Ø¹Ø©', 'Ù„Ù…Ø¯Ø©', 'ÙŠÙˆÙ…ÙŠØ§', 'Ø¨Ø¹Ø¯', 'ÙÙ‚Ø·', 'ØµØ¨Ø§Ø­Ø§', 'Ø¹Ù†Ø¯', 'Ù¡Ù¥' ,'Ø£ØªØ±ÙˆØ¨ÙŠÙ†', 'Ø£Ø²ÙŠØ«Ø±ÙˆÙ…ÙŠØ³ÙŠÙ†', 'Ø£Ø³ÙŠØªØ§Øª Ø§Ù„Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†', 'Ø£Ø³ÙŠØªØ§Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯', 'Ø£Ø³ÙŠØªÙŠÙ„ Ø³ÙŠØ³ØªÙŠÙ†', 'Ø£Ù„Ø¨ÙŠÙ†Ø¯Ø§Ø²ÙˆÙ„', 'Ø£Ù…Ø§Ù†ØªØ§Ø¯ÙŠÙ†', 'Ø£Ù…Ù„Ø§Ø­ Ø§Ù„Ø¥Ù…Ø§Ù‡Ø© Ø§Ù„ÙÙ…ÙˆÙŠØ©', 'Ø£Ù…ÙˆÙƒØ³ÙŠØ³ÙŠÙ„ÙŠÙ†', 'Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ø£ÙˆÙ…ÙŠØ¨Ø±Ø§Ø²ÙˆÙ„', 'Ø£ÙˆÙ†Ø¯Ø§Ù†Ø³ÙŠØªØ±ÙˆÙ†', 'Ø¥Ø±ÙŠØ«Ø±ÙˆÙ…Ø§ÙŠØ³ÙŠÙ†', 'Ø¥ÙŠØ¨ÙˆØ¨Ø±ÙˆÙÙŠÙ†', 'Ø§Ù„Ø¨Ø¨ØªÙŠØ¯Ø§Øª', 'Ø§Ù„Ø¨Ø±ÙˆØ¨ÙŠÙˆØªÙŠÙƒ', 'Ø§Ù„Ø¨ÙˆØªÙˆÙƒØ³', 'Ø§Ù„ÙÙŠØªØ§Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©', 'Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„', 'Ø¨Ø±Ø§Ù…ÙŠØ¨ÙŠÙƒØ³ÙˆÙ„', 'Ø¨Ø±ÙŠØ¬Ø§Ø¨Ø§Ù„ÙŠÙ†', 'Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†', 'Ø¨Ø±ÙŠÙ…ÙˆÙ†ÙŠØ¯ÙŠÙ†', 'Ø¨ÙŠÙ„ÙˆÙƒØ§Ø±Ø¨ÙŠÙ†', 'Ø¨ÙŠÙ…Ø§ØªÙˆØ¨Ø±ÙˆØ³Øª', 'ØªØ±Ø§ÙÙˆØ¨Ø±ÙˆØ³Øª', 'ØªØ±ÙˆØ¨ÙŠÙƒØ§Ù…ÙŠØ¯', 'ØªØ±ÙŠØªÙŠÙ†ÙˆÙŠÙ†', 'ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ†', 'ØªÙˆØ¨ÙŠØ±Ø§Ù…ÙŠØª', 'ØªÙŠÙ…ÙˆÙ„ÙˆÙ„', 'Ø¬Ø§Ø¨Ø§Ø¨Ù†ØªÙŠÙ†', 'Ø¬Ø§ØªÙŠÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ø­Ù…Ø¶ Ø§Ù„Ø¬Ù„ÙŠÙƒÙˆÙ„ÙŠÙƒ', 'Ø­Ù…Ø¶ Ø§Ù„Ø³Ø§Ù„ÙŠØ³ÙŠÙ„ÙŠÙƒ', 'Ø­Ù…Ø¶ Ø§Ù„Ù„Ø§ÙƒØªÙŠÙƒ', 'Ø­Ù…Ø¶ Ø§Ù„Ù‡ÙŠØ§Ù„ÙˆØ±ÙˆÙ†ÙŠÙƒ', 'Ø¯Ù…ÙˆØ¹ ØµÙ†Ø§Ø¹ÙŠØ©', 'Ø¯ÙˆØ±Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯', 'Ø¯ÙˆÙƒØ³ÙŠØ³ÙŠÙƒÙ„ÙŠÙ†', 'Ø¯ÙˆÙ…Ø¨ÙŠØ±ÙŠØ¯ÙˆÙ†', 'Ø¯ÙˆÙ†ÙŠØ¨ÙŠØ²ÙŠÙ„', 'Ø¯ÙŠÙƒØ³Ø§Ù…ÙŠØ«Ø§Ø²ÙˆÙ†', 'Ø¯ÙŠÙƒØ³ØªØ±ÙˆÙ…ÙŠØ«ÙˆØ±ÙØ§Ù†', 'Ø¯ÙŠÙƒÙ„ÙˆÙÙŠÙ†Ø§Ùƒ', 'Ø±Ø§Ù†ÙŠØªÙŠØ¯ÙŠÙ†', 'Ø±ÙˆØ¨ÙŠÙ†ÙŠØ±ÙˆÙ„', 'Ø±ÙŠØªÙŠÙ†ÙˆÙ„', 'Ø±ÙŠÙØ§Ø³ØªØ¬Ù…ÙŠÙ†', 'Ø³Ø§Ù„Ø¨ÙŠÙˆØªØ§Ù…ÙˆÙ„', 'Ø³ÙŠØ¨Ø±ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ø³ÙŠØªØ±ÙŠØ²ÙŠÙ†', 'Ø³ÙŠÙØ¯ÙŠÙ†ÙŠØ±', 'Ø³ÙŠÙÙˆØ±ÙŠÙˆÙƒØ³ÙŠÙ…', 'Ø³ÙŠÙÙŠÙƒØ³ÙŠÙ…', 'Ø³ÙŠÙƒÙ„ÙˆØ³Ø¨ÙˆØ±ÙŠÙ†', 'ØºÙˆØ§ÙŠÙÙŠÙ†ÙŠØ³ÙŠÙ†', 'ÙØ§Ù„Ø¨Ø±ÙˆØ§Øª', 'ÙÙ„ÙˆØ±ÙˆÙ…ÙŠØ«ÙˆÙ„ÙˆÙ†', 'ÙÙˆØ±ÙŠÙƒÙˆÙ†Ø§Ø²ÙˆÙ„', 'ÙÙŠØªØ§Ù…ÙŠÙ† Ø¯', 'Ù‚Ø·Ø±Ø§Øª Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†', 'ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„', 'ÙƒØ§Ø±Ø¨Ø§Ù…Ø§Ø²ÙŠØ¨ÙŠÙ†', 'ÙƒØ§Ø±Ø¨ÙˆØ³ÙŠØ³ØªÙŠÙ†', 'ÙƒØ¨Ø±ÙŠØªØ§Øª Ø§Ù„Ø²Ù†Ùƒ', 'ÙƒØ±Ø¨ÙˆÙ†Ø§Øª Ø§Ù„ÙƒØ§Ù„Ø³ÙŠÙˆÙ…', 'ÙƒÙ„ÙˆØ±Ø§Ù…ÙÙŠÙ†ÙŠÙƒÙˆÙ„', 'ÙƒÙ„ÙˆØ±Ù‡ÙŠÙƒØ³ÙŠØ¯ÙŠÙ†', 'ÙƒÙ„ÙŠÙ†Ø¯Ø§Ù…Ø§ÙŠØ³ÙŠÙ†', 'ÙƒÙˆÙ„Ø§Ø¬ÙŠÙ†', 'ÙƒÙŠØªÙˆØ±ÙˆÙ„Ø§Ùƒ', 'Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª', 'Ù„ÙˆØ±Ø§ØªØ§Ø¯ÙŠÙ†', 'Ù„ÙŠØ¯ÙˆÙƒØ§ÙŠÙŠÙ†', 'Ù„ÙŠÙÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ù„ÙŠÙÙŠØªÙŠØ¬Ø±Ø§Ø³Øª', 'Ù„ÙŠÙÙŠØªÙŠØ±Ø§Ø³ÙŠØªØ§Ù…', 'Ù…Ø§Ù†ÙŠØªÙˆÙ„', 'Ù…ØµÙ„ ÙÙŠØªØ§Ù…ÙŠÙ† Ø³ÙŠ', 'Ù…ÙƒÙ…Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙŠØ¯', 'Ù…ÙˆØ¨ÙŠØ±ÙˆØ³ÙŠÙ†', 'Ù…ÙˆÙƒØ³ÙŠÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†', 'Ù…ÙˆÙ†ØªÙŠÙ„ÙˆÙƒØ§Ø³Øª', 'Ù…ÙŠØ¨ÙŠÙ†Ø¯Ø§Ø²ÙˆÙ„', 'Ù…ÙŠØªØ±ÙˆÙ†ÙŠØ¯Ø§Ø²ÙˆÙ„', 'Ù…ÙŠØªÙˆÙƒÙ„ÙˆØ¨Ø±Ø§Ù…ÙŠØ¯', 'Ù…ÙŠÙ…Ø§Ù†ØªÙŠÙ†', 'Ù†Ø§ØªØ§Ù…ÙŠØ³ÙŠÙ†', 'Ù†ÙŠØ§Ø³ÙŠÙ†Ø§Ù…ÙŠØ¯', 'Ù†ÙŠØ¨Ø§ÙÙŠÙ†Ø§Ùƒ', 'Ù†ÙŠØ³ØªØ§ØªÙŠÙ†', 'Ù‡ÙŠØ§Ù„ÙˆØ±ÙˆÙ†Ø§Øª Ø§Ù„ØµÙˆØ¯ÙŠÙˆÙ…', 'Ù‡ÙŠØ¯Ø±ÙˆÙƒØ³ÙŠØ²ÙŠÙ†', 'Ù‡ÙŠØ¯Ø±ÙˆÙƒÙˆØ±ØªÙŠØ²ÙˆÙ†', 'Ù‡ÙŠØ¯Ø±ÙˆÙƒÙŠÙ†ÙˆÙ†']
EnglishWords=['Acetazolamide', 'Acetylcysteine', 'Albendazole', 'Amantadine', 'Amoxicillin', 'Artificial Tears', 'Atropine', 'Azithromycin', 'Bimatoprost', 'Botox', 'Brimonidine', 'Calcium Carbonate', 'Carbachol', 'Carbamazepine', 'Carbocisteine', 'Cefdinir', 'Cefixime', 'Cefuroxime', 'Cetirizine', 'Chloramphenicol', 'Chlorhexidine', 'Ciprofloxacin', 'Clindamycin', 'Collagen', 'Cyclosporine', 'Dexamethasone', 'Dextromethorphan', 'Diclofenac', 'Domperidone', 'Donepezil', 'Dorzolamide', 'Doxycycline', 'Erythromycin', 'Fluorometholone', 'Gabapentin', 'Gatifloxacin', 'Glycolic Acid', 'Guaifenesin', 'Hyaluronic Acid', 'Hydrocortisone', 'Hydroquinone', 'Hydroxyzine', 'Ibuprofen', 'Iron Supplements', 'Ketorolac', 'Lactic Acid', 'Latanoprost', 'Levetiracetam', 'Levofloxacin', 'Lidocaine', 'Lifitegrast', 'Loratadine', 'Mannitol', 'Mebendazole', 'Memantine', 'Metoclopramide', 'Metronidazole', 'Montelukast', 'Moxifloxacin', 'Multivitamins', 'Mupirocin', 'Natamycin', 'Nepafenac', 'Niacinamide', 'Nystatin', 'Ofloxacin', 'Omeprazole', 'Ondansetron', 'Oral Rehydration Salts', 'Paracetamol', 'Peptides', 'Pilocarpine', 'Pramipexole', 'Prednisolone', 'Prednisolone Acetate', 'Prednisolone Drops', 'Pregabalin', 'Probiotics', 'Ranitidine', 'Retinol', 'Rivastigmine', 'Ropinirole', 'Salbutamol', 'Salicylic Acid', 'Sodium Hyaluronate', 'Timolol', 'Tobramycin', 'Topiramate', 'Travoprost', 'Tretinoin', 'Tropicamide', 'Valproate', 'Vitamin C Serum', 'Vitamin D', 'Voriconazole', 'Zinc Sulfate''for', 'Every', 'After', 'Twice', 'needed', 'first', 'hours', 'breakfast', 'For', '8', 'awake', 'other', 'Night', 'hour', 'Morning', 'bed', '14', 'the', 'days', 'As', '5', 'bedtime', 'lunch', 'week', '7', 'daily', 'only', '4', 'Before', 'while', '3', 'minutes', '2', '6', 'day', '15', 'meals', 'Once', '12']

frequency_patterns = [
    ## ðŸ”¹ English Frequencies
    r"\bevery\s\d+\shours?\b",            # "every 6 hours"
    r"\bEvery\shours\b",
    r"\bevery\s\d+\s?-\s?\d+\shours?\b",  # "every 4-6 hours"
    r"\bonce\sdaily\b",                   # "once daily"
    r"\btwice\sdaily\b",                  # "twice daily"
    r"\bthree\stimes\sa\sday\b",          # "three times a day"
    r"\bfour\stimes\sa\sday\b",           # "four times a day"
    r"\b\d+\stimes\sa\sday\b",            # "5 times a day"

    ## â³ Time-Based
    r"\bevery\sother\sday\b",             # "every other day"
    r"\bevery\s\d+\sdays?\b",             # "every 3 days"
    r"\bevery\s\d+\sweeks?\b",            # "every 2 weeks"
    r"\bevery\s\d+\smonths?\b",           # "every 6 months"

    ## ðŸŒ™ Morning/Evening
    r"\bin\sthe\smorning\b",              # "in the morning"
    r"\bin\sthe\sevening\b",              # "in the evening"
    r"\bin\sthe\safternoon\b",            # "in the afternoon"
    r"\bin\sthe\snight\b",                # "in the night"
    r"\bdaily\sat\snoon\b",               # "daily at noon"

    ## ðŸ½ Meal-Based
    r"\bbefore\smeals?\b",                # "before meals"
    r"\bbefore\sbreakfast?\b",            # "before breakfast"
    r"\bafter\smeals?\b",                 # "after meals"
    r"\bafter\sbreakfast?\b",             # "after breakfast"
    r"\bbefore\sfood\b",                  # "before food"
    r"\bafter\sfood\b",                   # "after food"
    r"\bon\san\sempty\sstomach\b",        # "on an empty stomach"

    ## ðŸŒ™ Sleep
    r"\bbefore\sbedtime\b",               # "before bedtime"
    r"\bat\sbedtime\b",                   # "at bedtime"
    r"\bbefore\sgoing\sto\sbed\b",        # "before going to bed"

    ## ðŸ”„ PRN (As Needed)
    r"\bas\sneeded\b",                    # "as needed"
    r"\bif\sneeded\b",                    # "if needed"
    r"\bwhen\snecessary\b",               # "when necessary"
    r"\bwhen\srequired\b",                # "when required"
    r"\bwhen\sfeeling\spain\b",           # "when feeling pain"

    ## ðŸš‘ Perioperative
    r"\bbefore\ssurgery\b",               # "before surgery"
    r"\bafter\ssurgery\b",                # "after surgery"
    r"\bbefore\san\soperation\b",         # "before an operation"
    r"\bafter\san\soperation\b",          # "after an operation"

    ## ðŸ”¹ Arabic Frequencies
    r"\bÙƒÙ„\s\d+\sØ³Ø§Ø¹Ø©\b",              # "ÙƒÙ„ 8 Ø³Ø§Ø¹Ø§Øª" (every X hours)
    r"\bÙƒÙ„\s\d+\s?-\s?\d+\sØ³Ø§Ø¹Ø§Øª?\b",  # "ÙƒÙ„ 4-6 Ø³Ø§Ø¹Ø§Øª" (every X-Y hours)
    r"\bÙ…Ø±Ø©\sÙŠÙˆÙ…ÙŠØ§\b",                 # "Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠÙ‹Ø§" (once daily)
    r"\bÙ…Ø±Ø©\sÙƒÙ„\sÙŠÙˆÙ…\b",               # "Ù…Ø±Ø© ÙƒÙ„ ÙŠÙˆÙ…" (once per day)
    r"\bÙ…Ø±Ø©\sÙŠÙˆÙ…ÙŠØ§\b",                 # "Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§" (once per day)
    r"\bÙ…Ø±Ø©\sØ£Ø³Ø¨ÙˆØ¹ÙŠØ§\b",               # "Ù…Ø±Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ‹Ø§" (once weekly)
    r"\bÙ…Ø±Ø©\sÙƒÙ„\sØ£Ø³Ø¨ÙˆØ¹\b",             # "Ù…Ø±Ø© ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹" (once per week)
    r"\bÙ…Ø±Ø©\sØ´Ù‡Ø±ÙŠØ§\b",                 # "Ù…Ø±Ø© Ø´Ù‡Ø±ÙŠÙ‹Ø§" (once monthly)
    r"\bÙ…Ø±Ø©\sÙƒÙ„\sØ´Ù‡Ø±\b",               # "Ù…Ø±Ø© ÙƒÙ„ Ø´Ù‡Ø±" (once per month)
    r"\bÙ…Ø±ØªÙŠÙ†\sÙŠÙˆÙ…ÙŠØ§\b",               # "Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠÙ‹Ø§" (twice daily)
    r"\b\d+\sÙ…Ø±Ø§Øª?\sÙŠÙˆÙ…ÙŠØ§\b",          # "3 Ù…Ø±Ø§Øª ÙŠÙˆÙ…ÙŠÙ‹Ø§" (multiple times daily)

    ## â³ Time-Based
    r"\bÙƒÙ„\sÙŠÙˆÙ…ÙŠÙ†\b",                   # "ÙƒÙ„ ÙŠÙˆÙ…ÙŠÙ†" (every other day)
    r"\bÙƒÙ„\s\d+\sØ£ÙŠØ§Ù…\b",               # "ÙƒÙ„ 3 Ø£ÙŠØ§Ù…" (every X days)
    r"\bÙƒÙ„\s\d+\sØ£Ø³Ø§Ø¨ÙŠØ¹\b",             # "ÙƒÙ„ 2 Ø£Ø³Ø§Ø¨ÙŠØ¹" (every X weeks)
    r"\bÙƒÙ„\s\d+\sØ´Ù‡ÙˆØ±\b",               # "ÙƒÙ„ 6 Ø´Ù‡ÙˆØ±" (every X months)

   ## Arabic (Word-Based Numbers)
    r"\bÙƒÙ„\sØ³Ø§Ø¹Ø©\b",                     # "ÙƒÙ„ ÙˆØ§Ø­Ø¯Ø© Ø³Ø§Ø¹Ø©" (every one hour)
    r"\bÙƒÙ„\sØ³Ø§Ø¹ØªÙŠÙ†\b",                   # "ÙƒÙ„ Ø§Ø«Ù†ØªÙŠÙ† Ø³Ø§Ø¹Ø©" (every two hours)
    r"\bÙƒÙ„\sØ«Ù„Ø§Ø«\sØ³Ø§Ø¹Ø§Øª\b",               # "ÙƒÙ„ Ø«Ù„Ø§Ø« Ø³Ø§Ø¹Ø§Øª" (every three hours)
    r"\bÙƒÙ„\sØ£Ø±Ø¨Ø¹\sØ³Ø§Ø¹Ø§Øª\b",              # "ÙƒÙ„ Ø£Ø±Ø¨Ø¹ Ø³Ø§Ø¹Ø§Øª" (every four hours)
    r"\bÙƒÙ„\sØ®Ù…Ø³\sØ³Ø§Ø¹Ø§Øª\b",               # "ÙƒÙ„ Ø®Ù…Ø³ Ø³Ø§Ø¹Ø§Øª" (every five hours)
    r"\bÙƒÙ„\sØ³Øª\sØ³Ø§Ø¹Ø§Øª\b",                # "ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª" (every six hours)
    r"\bÙƒÙ„\sØ³Ø¨Ø¹\sØ³Ø§Ø¹Ø§Øª\b",               # "ÙƒÙ„ Ø³Ø¨Ø¹ Ø³Ø§Ø¹Ø§Øª" (every seven hours)
    r"\bÙƒÙ„\sØ«Ù…Ø§Ù†ÙŠ\sØ³Ø§Ø¹Ø§Øª\b",             # "ÙƒÙ„ Ø«Ù…Ø§Ù†ÙŠ Ø³Ø§Ø¹Ø§Øª" (every eight hours)
    r"\bÙƒÙ„\sØªØ³Ø¹\sØ³Ø§Ø¹Ø§Øª\b",               # "ÙƒÙ„ ØªØ³Ø¹ Ø³Ø§Ø¹Ø§Øª" (every nine hours)
    r"\bÙƒÙ„\sØ¹Ø´Ø±\sØ³Ø§Ø¹Ø§Øª\b",               # "ÙƒÙ„ Ø¹Ø´Ø± Ø³Ø§Ø¹Ø§Øª" (every ten hours)
    r"\bÙƒÙ„\sØ¥Ø­Ø¯Ù‰\sØ¹Ø´Ø±Ø©\sØ³Ø§Ø¹Ø©\b",         # "ÙƒÙ„ Ø¥Ø­Ø¯Ù‰ Ø¹Ø´Ø±Ø© Ø³Ø§Ø¹Ø©" (every 11 hours)
    r"\bÙƒÙ„\sØ§Ø«Ù†ØªÙŠ\sØ¹Ø´Ø±Ø©\sØ³Ø§Ø¹Ø©\b",        # "ÙƒÙ„ Ø§Ø«Ù†ØªÙŠ Ø¹Ø´Ø±Ø© Ø³Ø§Ø¹Ø©" (every 12 hours)

    ## ðŸŒ™ Morning/Evening
    r"\bÙÙŠ\sØ§Ù„ØµØ¨Ø§Ø­\b",                  # "ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­" (in the morning)
    r"\bÙÙŠ\sØ§Ù„Ù…Ø³Ø§Ø¡\b",                  # "ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¡" (in the evening)
    r"\bÙÙŠ\sØ§Ù„Ø¸Ù‡ÙŠØ±Ø©\b",                 # "ÙÙŠ Ø§Ù„Ø¸Ù‡ÙŠØ±Ø©" (at noon)
    r"\bÙÙŠ\sØ§Ù„Ù„ÙŠÙ„\b",                   # "ÙÙŠ Ø§Ù„Ù„ÙŠÙ„" (at night)

    ## ðŸ½ Meal-Based
    r"\bÙ‚Ø¨Ù„\sØ§Ù„Ø£ÙƒÙ„\b",                  # "Ù‚Ø¨Ù„ Ø§Ù„Ø£ÙƒÙ„" (before meals)
    r"\bØ¨Ø¹Ø¯\sØ§Ù„Ø£ÙƒÙ„\b",                  # "Ø¨Ø¹Ø¯ Ø§Ù„Ø£ÙƒÙ„" (after meals)
    r"\bÙ‚Ø¨Ù„\sØ§Ù„Ø·Ø¹Ø§Ù…\b",                 # "Ù‚Ø¨Ù„ Ø§Ù„Ø·Ø¹Ø§Ù…" (before food)
    r"\bØ¨Ø¹Ø¯\sØ§Ù„Ø·Ø¹Ø§Ù…\b",                 # "Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø¹Ø§Ù…" (after food)
    r"\bØ¹Ù„Ù‰\sÙ…Ø¹Ø¯Ø©\sÙØ§Ø±ØºØ©\b",            # "Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ø© ÙØ§Ø±ØºØ©" (on an empty stomach)
    r"\bØ¹Ù„Ù‰\sØ§Ù„Ø±ÙŠÙ‚\b",                   # "Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙŠÙ‚" (fasting)

    ## ðŸŒ™ Sleep
    r"\bÙ‚Ø¨Ù„\sØ§Ù„Ù†ÙˆÙ…\b",                 # "Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…" (before sleep)
    r"\bØ¹Ù†Ø¯\sØ§Ù„Ù†ÙˆÙ…\b",                 # "Ø¹Ù†Ø¯ Ø§Ù„Ù†ÙˆÙ…" (at bedtime)

    ## ðŸ”„ PRN (As Needed)
    r"\bØ¹Ù†Ø¯\sØ§Ù„Ù„Ø²ÙˆÙ…\b",                # "Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…" (as needed)
    r"\bØ­Ø³Ø¨\sØ§Ù„Ø­Ø§Ø¬Ø©\b",                # "Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©" (as required)
    r"\bØ¥Ø°Ø§\sØ§Ø³ØªØ¯Ø¹Øª\sØ§Ù„Ø­Ø§Ø¬Ø©\b",         # "Ø¥Ø°Ø§ Ø§Ø³ØªØ¯Ø¹Øª Ø§Ù„Ø­Ø§Ø¬Ø©" (if necessary)
    r"\bØ¹Ù†Ø¯\sØ§Ù„Ø´Ø¹ÙˆØ±\sØ¨Ø§Ù„Ø£Ù„Ù…\b",          # "Ø¹Ù†Ø¯ Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø£Ù„Ù…" (when feeling pain)

    ## ðŸš‘ Perioperative
    r"\bÙ‚Ø¨Ù„\sØ§Ù„Ø¹Ù…Ù„ÙŠØ©\b",              # "Ù‚Ø¨Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©" (before surgery)
    r"\bØ¨Ø¹Ø¯\sØ§Ù„Ø¹Ù…Ù„ÙŠØ©\b",              # "Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©" (after surgery)
    r"\bÙ‚Ø¨Ù„\sØ§Ù„ØªØ¯Ø®Ù„\sØ§Ù„Ø¬Ø±Ø§Ø­ÙŠ\b",     # "Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ø¬Ø±Ø§Ø­ÙŠ" (before an operation)
    r"\bØ¨Ø¹Ø¯\sØ§Ù„ØªØ¯Ø®Ù„\sØ§Ù„Ø¬Ø±Ø§Ø­ÙŠ\b",     # "Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ø¬Ø±Ø§Ø­ÙŠ" (after an operation)
    r"\bEvery\s\d+\shours?\b",            # "Every 8 hours" (capitalized)
    r"\bEvery\shours?\b",                 # "Every hours" (capitalized)
    r"\bAfter\slunch\b",                  # "After lunch" (capitalized)
    r"\bOnce\sdaily\b",                   # "Once daily" (capitalized)

]

"""# **TR OCR MODEL**

## **load model with weights**
"""

# from google.colab import drive
# drive.mount('/content/drive')
# import zipfile
# import gdown

# # Download the zip file from Google Drive
# zip_path = 'https://drive.google.com/uc?id=1lzbd9t9aJ-NJYCS__8uzCpObjbsN5Tm2&export=download'  # Corrected URL for downloading
# output_path = '/content/model.zip'  # Path to save the downloaded zip file
# gdown.download(zip_path, output_path, quiet=False)

# extract_path = '/content/model_dir'

# # Open and extract the downloaded zip file
# with zipfile.ZipFile(output_path, 'r') as zip_ref: # Changed to use downloaded zip file path
#     zip_ref.extractall(extract_path)

from transformers import VisionEncoderDecoderModel, TrOCRProcessor
import torch
import os

MODEL_NAME = "microsoft/trocr-base-handwritten"
processor = TrOCRProcessor.from_pretrained(MODEL_NAME) # Initialize the processor here
new_model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)
new_model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
new_model.config.pad_token_id = processor.tokenizer.pad_token_id
new_model.load_state_dict(torch.load("/kaggle/input/trocr_finetune_weights_stp/pytorch/default/1/model_weights.pth"))
print("Model weights loaded successfully into a new model.")

"""# **Define Functions**

## **vision functions**
"""

from sklearn.cluster import DBSCAN
def fuzzy_match(word, word_list, threshold=30):
    result = process.extractOne(word, word_list, scorer=fuzz.ratio)
    if result:
        # Handle different versions of fuzzywuzzy
        if isinstance(result, tuple) and len(result) >= 2:
            match, score = result[0], result[1]  # Extract match and score
            return match if score >= threshold else None
    return None

def remove_duplicate_boxes(boxes, iou_threshold=0.8):
    if len(boxes) == 0:
        return []

    # Convert boxes to (x1, y1, x2, y2) format
    boxes = np.array(boxes)
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    # Compute the area of the bounding boxes
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)

    # Sort the boxes by the bottom-right y-coordinate
    indices = np.argsort(y2)
    keep = []

    while len(indices) > 0:
        last = len(indices) - 1
        i = indices[last]
        keep.append(i)

        # Compute the IoU of the remaining boxes with the last box
        xx1 = np.maximum(x1[i], x1[indices[:last]])
        yy1 = np.maximum(y1[i], y1[indices[:last]])
        xx2 = np.minimum(x2[i], x2[indices[:last]])
        yy2 = np.minimum(y2[i], y2[indices[:last]])

        w = np.maximum(0, xx2 - xx1 + 1)
        h = np.maximum(0, yy2 - yy1 + 1)
        overlap = (w * h) / areas[indices[:last]]

        # Remove boxes with IoU greater than the threshold
        indices = np.delete(indices, np.concatenate(([last], np.where(overlap > iou_threshold)[0])))

    return boxes[keep].tolist()

def sort_boxes_top_to_bottom_left_to_right(boxes):
    if len(boxes) == 0:
        return []

    # Convert boxes to numpy array
    boxes = np.array(boxes)
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]

    # Sort the boxes by the top-left y-coordinate (y1), then by the top-left x-coordinate (x1)
    indices = np.lexsort((x1, y1))  # Sort by y1 first, then by x1
    return boxes[indices].tolist()

def preprocess_image(image):
        # img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        img_blur = cv2.GaussianBlur(gray, (5, 5), 0)
        img_thresh = cv2.adaptiveThreshold(img_blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 101, 20)
        kernel = np.ones((2, 2), np.uint8)
        img_morph = cv2.morphologyEx(img_thresh, cv2.MORPH_CLOSE, kernel, iterations=1)
        clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))
        img_clahe = clahe.apply(img_morph)
        return img_clahe

# -----------------------------------
def group_boxes_into_lines_and_sort(boxes, texts, vertical_threshold=20):
    if not boxes or not texts:
        return [], []

    # Convert boxes to include center points and other useful metrics
    box_data = []
    for i, (box, text) in enumerate(zip(boxes, texts)):
        if text:  # Only process boxes with valid text
            x1, y1, x2, y2 = box
            center_y = (y1 + y2) / 2
            center_x = (x1 + x2) / 2
            height = y2 - y1
            box_data.append({
                'index': i,
                'box': box,
                'text': text,
                'center_x': center_x,
                'center_y': center_y,
                'height': height
            })

    if not box_data:
        return [], []

    # Dynamically adjust vertical threshold based on average text height
    avg_height = sum(item['height'] for item in box_data) / len(box_data)
    dynamic_threshold = max(vertical_threshold, avg_height * 0.7)

    # Initial sort by y-coordinate
    box_data.sort(key=lambda x: x['center_y'])

    # Use DBSCAN clustering to group lines
    points = np.array([[item['center_x'], item['center_y']] for item in box_data])
    # Adjust eps based on average height
    clustering = DBSCAN(eps=dynamic_threshold, min_samples=1).fit(points)
    labels = clustering.labels_

    # Group boxes by cluster label
    lines = {}
    for i, label in enumerate(labels):
        if label not in lines:
            lines[label] = []
        lines[label].append(box_data[i])

    # Sort lines by average y-coordinate
    sorted_lines = sorted(lines.values(), key=lambda line: sum(item['center_y'] for item in line)/len(line))

    # Sort each line by x-coordinate (left to right)
    for line in sorted_lines:
        line.sort(key=lambda x: x['center_x'])

    # Extract the ordered text and boxes
    ordered_text = []
    ordered_boxes = []
    for line in sorted_lines:
        for box in line:
            ordered_text.append(box['text'])
            ordered_boxes.append(box['box'])

    return ordered_text, ordered_boxes

"""## **Text Extraction Function**"""

from PIL import Image
import cv2
import torch
from transformers import TrOCRProcessor, VisionEncoderDecoderModel

def TextExtraction(images):
    extracted_data = []  # List to store results per input image

    for image_path in images:
        # Load image
        image = cv2.imread(image_path)
        image_data = {
            "image_path": image_path,
            "words": []  # List to store {text, position} for each word
        }

        # Run YOLOv8 detection
        results = model(image)
        boxes = results[0].boxes.xyxy.cpu().numpy()  # Bounding boxes
        class_ids = results[0].boxes.cls.cpu().numpy()  # Class IDs (0=Arabic, 1=English)

        # Remove duplicate boxes and sort
        boxes = remove_duplicate_boxes(boxes)
        sorted_boxes = sort_boxes_top_to_bottom_left_to_right(boxes)

        for i, box in enumerate(sorted_boxes):
            x1, y1, x2, y2 = map(int, box)
            cropped_image = image[y1:y2, x1:x2]

            # Preprocess for TrOCR
            trocr_input = preprocess_for_trocr(cropped_image)
            inputs = processor(images=trocr_input, return_tensors="pt").to(new_model.device)

            # Extract text
            with torch.no_grad():
                generated_ids = new_model.generate(**inputs)
                output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Fuzzy match with language-specific dictionary
            class_id = int(class_ids[i])
            word_list = ArabicWords if class_id == 0 else EnglishWords
            matched_text = fuzzy_match(output_text.strip(), word_list)

            # Save text + coordinates
            image_data["words"].append({
                "text": matched_text,
                "position": (x1, y1, x2, y2)  # (left, top, right, bottom)
            })

        extracted_data.append(image_data)

    return extracted_data

def TextExtraction(images):
    extracted_data = []
    for image_path in images:
        image = cv2.imread(image_path)
        image_data = {
            "image_path": image_path,
            "text": "",  # Full text in reading order
            "words": []  # List to store {text, position} for each word
        }

        # Run YOLOv8 detection
        results = model(image)
        boxes = results[0].boxes.xyxy.cpu().numpy()
        class_ids = results[0].boxes.cls.cpu().numpy()

        # Remove duplicate boxes
        boxes = remove_duplicate_boxes(boxes)

        # Extract text from each box
        all_texts = []
        for i, box in enumerate(boxes):
            x1, y1, x2, y2 = map(int, box)
            cropped_image = image[y1:y2, x1:x2]

            # Preprocess for TrOCR
            trocr_input = preprocess_for_trocr(cropped_image)
            inputs = processor(images=trocr_input, return_tensors="pt").to(new_model.device)

            # Extract text
            with torch.no_grad():
                generated_ids = new_model.generate(**inputs)
                output_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

            # Fuzzy match with language-specific dictionary
            class_id = int(class_ids[i])
            word_list = ArabicWords if class_id == 0 else EnglishWords
            matched_text = fuzzy_match(output_text.strip(), word_list)
            if matched_text:
                all_texts.append(matched_text)
                # Save the original box for later
                boxes[i] = box  # Ensure box is in the right format

        # Group into lines and sort in reading order
        ordered_texts, ordered_boxes = group_boxes_into_lines_and_sort(boxes, all_texts)

        # Build the complete text and word objects
        full_text = " ".join(ordered_texts)
        image_data["text"] = full_text

        for text, box in zip(ordered_texts, ordered_boxes):
            x1, y1, x2, y2 = map(int, box)
            image_data["words"].append({
                "text": text,
                "position": (x1, y1, x2, y2)
            })

        extracted_data.append(image_data)

    return extracted_data

import pandas as pd
df=pd.read_excel("/kaggle/input/merge-file/merge_file.xlsx")

df.head()

import pandas as pd
import re

def extract_arabic_english_words(df, column_name):
    arabic_words = set()
    english_words = set()

    for entry in df[column_name].dropna():
        # Normalize separators (Arabic comma and normal comma) and split
        parts = re.split(r'[ØŒ,]', str(entry))
        for phrase in parts:
            # Extract all words (Arabic + English/numbers)
            words = re.findall(r'\b[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FFa-zA-Z0-9]+\b', phrase)
            for word in words:
                word = word.strip()
                if not word:
                    continue
                # Check if word is Arabic
                if re.search(r'[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]', word):
                    arabic_words.add(word)
                else:
                    english_words.add(word)

    return list(arabic_words), list(english_words)

arabic_list, english_list = extract_arabic_english_words(df, 'Appointment')

print("Arabic Words:", arabic_list)
print("English Words:", english_list)

print(len(arabic_list))
print(len(english_list))

"""## **Phase 2 function**

-----------------------
"""

DEFAULT_FREQUENCY = "Every 6 hours"

def extract_medicine_and_frequency(sentence, medicine_list, frequency_patterns):
    extracted_medicines = []
    extracted_frequencies = []
    medicine_positions = {}
    frequency_positions = {}

    # Step 1: Extract medicines and their positions
    for med in medicine_list:
        match = re.search(rf"\b{re.escape(med)}\b", sentence)
        if match:
            extracted_medicines.append(med)
            medicine_positions[med] = match.start()

    # Step 2: Extract frequencies and their positions
    for pattern in frequency_patterns:
        pattern_re = re.compile(pattern)
        matches = pattern_re.finditer(sentence)
        for match in matches:
            extracted_frequencies.append(match.group())
            frequency_positions[match.group()] = match.start()

    # Step 3: Sort medicines and frequencies by their position
    sorted_medicines = sorted(medicine_positions.items(), key=lambda x: x[1])
    sorted_frequencies = sorted(frequency_positions.items(), key=lambda x: x[1])

    # Step 4: Pair medicines with frequencies
    extracted_results = []
    freq_index = 0
    last_known_frequency = None

    for i, (med, med_pos) in enumerate(sorted_medicines):
        extracted_frequency = "Unknown"
        next_med_pos = sorted_medicines[i + 1][1] if i + 1 < len(sorted_medicines) else len(sentence)

        # Check if there are words between this medicine and the next
        words_between = sentence[med_pos + len(med):next_med_pos].strip()
        if words_between and len(words_between.split()) <= 3:
            extracted_frequency = words_between
        else:
            # Try to match frequency after the medicine (if ordered correctly)
            while freq_index < len(sorted_frequencies):
                freq, freq_pos = sorted_frequencies[freq_index]
                if freq_pos > med_pos and freq_pos < next_med_pos:
                    extracted_frequency = freq
                    last_known_frequency = freq
                    freq_index += 1
                    break
                freq_index += 1

            # If no direct match, use last known frequency
            if extracted_frequency == "Unknown" and last_known_frequency:
                extracted_frequency = last_known_frequency

            # If still unknown, use default
            if extracted_frequency == "Unknown":
                extracted_frequency = DEFAULT_FREQUENCY

        extracted_results.append((med, extracted_frequency))

    return extracted_results

def sort_words_by_position(word_data):
    # Sort by y1 (top-to-bottom), then by x1 (left-to-right)
    return sorted(word_data, key=lambda x: (x["position"][1], x["position"][0]))

def print_sorted_words(sorted_words):
    print("Sorted Words (Top-to-Bottom, Left-to-Right):")
    for i, word in enumerate(sorted_words):
        print(f"{i+1}: '{word['text']}' at {word['position']}")

def pair_medicine_frequency(sorted_words):
    pairs = []
    i = 0
    n = len(sorted_words)

    while i < n:
        current = sorted_words[i]
        current_text = current['text']
        current_pos = current['position']

        # Check if current word is likely a medicine
        if is_medicine(current_text):
            medicine = current_text
            frequency_parts = []

            # Look ahead for frequency components
            j = i + 1
            while j < n:
                next_word = sorted_words[j]
                next_text = next_word['text']
                next_pos = next_word['position']

                # Check vertical alignment (same approximate line)
                if abs(next_pos[1] - current_pos[1]) > 50:  # Not on same line
                    break

                # Check if it's part of frequency
                if is_frequency(next_text):
                    frequency_parts.append(next_text)
                elif frequency_parts:  # Stop if we already have frequency parts
                    break

                j += 1

            if frequency_parts:
                frequency = ' '.join(frequency_parts)
                pairs.append((medicine, frequency))
                i = j  # Skip processed words
            else:
                i += 1
        else:
            i += 1

    return pairs

def is_medicine(text):
    """Check if text is likely a medicine name"""
    medicine_indicators = ['mg', 'ml', 'g', 'tablet', 'capsule', 'injection']
    return any(indicator in text.lower() for indicator in medicine_indicators)

def is_frequency(text):
    """Check if text is likely a frequency instruction"""
    frequency_terms = ['every', 'hours', 'daily', 'weekly', 'monthly',
                      'before', 'after', 'with', 'meal', 'meals',
                      'once', 'twice', 'thrice', 'morning', 'evening',
                      'night', 'bedtime', 'breakfast', 'lunch', 'dinner']
    return any(term in text.lower() for term in frequency_terms)

"""# **Submit Function**"""

def organize_medication_instructions(words_with_coords):
    # Step 1: Calculate the center y-coordinate for each word's bounding box
    words_with_y_center = []
    for word_dict in words_with_coords:
        word = word_dict['text']
        coords = word_dict['position']  # Assuming this is (x1, y1, x2, y2)
        x1, y1, x2, y2 = coords
        y_center = (y1 + y2) / 2
        x_center = (x1 + x2) / 2
        words_with_y_center.append((word, y_center, x_center, coords))

    # Step 2: Sort words by their y-coordinate (top to bottom)
    words_with_y_center.sort(key=lambda x: x[1])

    # Step 3: Group words that are likely on the same line
    # We'll consider words to be on the same line if their y-centers are within a threshold
    y_threshold = 100  # This can be adjusted based on your document's layout

    lines = []
    current_line = [words_with_y_center[0]]

    for i in range(1, len(words_with_y_center)):
        current_word = words_with_y_center[i]
        previous_word = words_with_y_center[i-1]

        # If this word is close in y-position to the previous word, add it to the current line
        if abs(current_word[1] - previous_word[1]) < y_threshold:
            current_line.append(current_word)
        # Otherwise, start a new line
        else:
            lines.append(current_line)
            current_line = [current_word]

    # Add the last line if it's not empty
    if current_line:
        lines.append(current_line)

    # Step 4: Sort words within each line by x-coordinate (left to right)
    for i in range(len(lines)):
        lines[i].sort(key=lambda x: x[2])  # Sort by x_center

    # Step 5: Join words in each line to form complete instructions
    instructions = []
    for line in lines:
        instruction = " ".join(word[0] for word in line)
        instructions.append(instruction)

    return instructions

def create_medication_frequency_pairs(organized_instructions):
    # Step 1: Concatenate all instructions into one line
    all_text = " ".join(organized_instructions)


    # Step 3: Extract medication-frequency pairs
    medication_pairs = []

    # Simple approach: Find a medication and assume text until the next medication is its frequency
    for i, med in enumerate(medicine_list):
        match = re.search(r'\b' + med + r'\b', all_text, re.IGNORECASE)
        if match:
            start_idx = match.start()

            # Find the next medication in the text after this one
            next_med_idx = float('inf')
            for other_med in medicine_list:
                if other_med != med:
                    other_match = re.search(r'\b' + other_med + r'\b', all_text[start_idx+len(med):], re.IGNORECASE)
                    if other_match:
                        if start_idx + len(med) + other_match.start() < next_med_idx:
                            next_med_idx = start_idx + len(med) + other_match.start()

            # Extract the frequency instruction
            if next_med_idx != float('inf'):
                frequency = all_text[start_idx+len(med):next_med_idx].strip()
            else:
                frequency = all_text[start_idx+len(med):].strip()

            medication_pairs.append((med, frequency))

    # Step 4: More sophisticated approach using patterns specific to medication instructions
    if not medication_pairs:
        # Pattern: Look for medication name followed by frequency info
        patterns = [
            # Medication followed by "Every X hours"
            (r'(\b\w+\b)\s+Every\s+(\d+)\s+hours', lambda m: (m.group(1), f"Every {m.group(2)} hours")),

            # Medication followed by "Once daily"
            (r'(\b\w+\b)\s+Once\s+daily', lambda m: (m.group(1), "Once daily")),

            # Medication followed by "After lunch"
            (r'(\b\w+\b)\s+After\s+lunch', lambda m: (m.group(1), "After lunch")),

            # More patterns can be added for different frequency formats
        ]

        for pattern, extract_func in patterns:
            for match in re.finditer(pattern, all_text, re.IGNORECASE):
                medication_pairs.append(extract_func(match))

    return medication_pairs


# medication_pairs = create_medication_frequency_pairs(organized_instructions)

# print("\nMedication-Frequency Pairs:")
# for med, freq in medication_pairs:
#     print(f"- {med}: {freq}")

def find_original_text(original_text, lowercase_match):
    pattern = re.compile(re.escape(lowercase_match), re.IGNORECASE)
    match = pattern.search(original_text)
    if match:
        return match.group(0)
    return lowercase_match

def extract_medication_frequency_pairs(text, medicine_list, frequency_patterns):
    """
    Extract medication and frequency pairs from text using predefined lists
    
    Args:
        text (str): Full text to extract from
        medicine_list (list): List of medicine names to look for
        frequency_patterns (list): List of frequency patterns to match
        
    Returns:
        list: Tuples of (medicine, frequency)
    """
    # Initialize results
    med_freq_pairs = []
    
    # Find all medicine names in the text
    found_medicines = []
    for med in medicine_list:
        matches = re.finditer(r'\b' + re.escape(med) + r'\b', text, re.IGNORECASE)
        for match in matches:
            found_medicines.append((med, match.start(), match.end()))
    
    # Sort medicines by their position in text
    found_medicines.sort(key=lambda x: x[1])
    
    # Find all frequency patterns
    found_frequencies = []
    for pattern in frequency_patterns:
        # Remove regex syntax if it's a regex pattern
        search_pattern = pattern
        if pattern.startswith(r'\b') and pattern.endswith(r'\b'):
            search_pattern = pattern[2:-2]
        
        # Try to find matches for this pattern
        try:
            for match in re.finditer(search_pattern, text, re.IGNORECASE):
                found_frequencies.append((match.group(), match.start(), match.end()))
        except re.error:
            # If pattern is invalid regex, try as literal text
            for match in re.finditer(re.escape(search_pattern), text, re.IGNORECASE):
                found_frequencies.append((match.group(), match.start(), match.end()))
    
    # Sort frequencies by position
    found_frequencies.sort(key=lambda x: x[1])
    
    # Match each medicine with appropriate frequency
    default_frequency = "Every 6 hours"  # Default if none found
    
    for i, (med, med_start, med_end) in enumerate(found_medicines):
        # Initialize with default
        closest_freq = default_frequency
        closest_dist = float('inf')
        
        # Find closest frequency that follows this medicine
        for freq, freq_start, freq_end in found_frequencies:
            if med_end < freq_start:  # Frequency appears after medicine
                distance = freq_start - med_end
                # If it's closer than current closest and before next medicine
                next_med_start = float('inf')
                if i + 1 < len(found_medicines):
                    next_med_start = found_medicines[i+1][1]
                
                if distance < closest_dist and freq_start < next_med_start:
                    closest_dist = distance
                    closest_freq = freq
        
        # Get original case for medication from text
        orig_med = find_original_text(text, med)
        
        # Add the pair (use original medication name from medicine_list if possible)
        med_freq_pairs.append((orig_med, closest_freq))
    
    return med_freq_pairs

def submit(images):
    results = []  # Stores the output for all prescriptions

    # Step 1: Extract text and positions from all images
    extracted_data = TextExtraction(images)

    for prescription_data in extracted_data:
        # Step 2: Sort words by position (top-to-bottom, left-to-right)
        sorted_words = sort_words_by_position(prescription_data["words"])

        # Optional: Print sorted words for debugging
        print(f"\nSorted words for {prescription_data['image_path']}:")
        print_sorted_words(sorted_words)
        
        # ----------------------------------------------
        print("-------------------------------------------------------------------------------")
        organized_instructions = organize_medication_instructions(sorted_words)

        print("Organized Medication Instructions:")
        for i, instruction in enumerate(organized_instructions, 1):
            print(f"{i}. {instruction}")
        print("-------------------------------------------------------------------------------")

        # Extract full text from organized instructions
        full_text = " ".join(organized_instructions)
        
        # Use the extract_medication_frequency_pairs function
        medication_pairs = extract_medication_frequency_pairs(full_text, medicine_list, frequency_patterns)

        print("\nMedication-Frequency Pairs:")
        for med, freq in medication_pairs:
            print(f"- {med}: {freq}")

        # Store results
        results.append({
            "image_path": prescription_data["image_path"],
            "prescription_pairs": medication_pairs
        })

    return results

# Example list of image file paths (update with actual paths)
images = ['/kaggle/input/machathon6-phase1-images/Dental_prescription_591.jpg', '/kaggle/input/machathon6-phase1-images/Neurology_prescription_131.jpg']

# Call the submit function
results = submit(images)

# Print the structured results
for i, result in enumerate(results):
    print(f"\nPrescription {i + 1} ({result['image_path']}):")
    if not result["prescription_pairs"]:
        print("  No medicine-frequency pairs found")
    else:
        for medicine, frequency in result["prescription_pairs"]:
            print(f"  Medicine: {medicine}, Frequency: {frequency}")

