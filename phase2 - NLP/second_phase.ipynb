{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # Show full content of each cell\n",
    "pd.set_option('display.expand_frame_repr', False)  # Avoid line wrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# medicine list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "medicine_list = [\n",
    "    # English Medicines\n",
    "    \"Cyclosporine\", \"Mebendazole\", \"Pilocarpine\", \"Calcium\", \"Hydrocortisone\", \"Memantine\",\n",
    "    \"Hydroquinone\", \"Loratadine\", \"Guaifenesin\", \"Retinol\", \"Hydroxyzine\", \"Carbocisteine\",\n",
    "    \"Montelukast\", \"Dexamethasone\", \"Niacinamide\", \"Cefdinir\", \"Albendazole\", \"Gabapentin\",\n",
    "    \"Levetiracetam\", \"Zinc\", \"Chlorhexidine\", \"Diclofenac\", \"Prednisolone\", \"Botox\",\n",
    "    \"Dextromethorphan\", \"Lidocaine\", \"Metronidazole\", \"Acetylcysteine\", \"Ciprofloxacin\",\n",
    "    \"Clindamycin\", \"Probiotics\", \"Erythromycin\", \"Paracetamol\", \"Omeprazole\",\n",
    "    \"Fluorometholone\", \"Azithromycin\", \"Nystatin\", \"Valproate\", \"Pramipexole\",\n",
    "    \"Carbamazepine\", \"Tretinoin\", \"Ofloxacin\", \"Tobramycin\", \"Timolol\", \"Ropinirole\",\n",
    "    \"Brimonidine\", \"Pregabalin\", \"Rivastigmine\", \"Amantadine\", \"Ranitidine\",\n",
    "    \"Salbutamol\", \"Domperidone\", \"Vitamin\", \"Dorzolamide\", \"Cetirizine\", \"Cefixime\",\n",
    "    \"Topiramate\", \"Iron\", \"Latanoprost\", \"Multivitamins\", \"Donepezil\", \"Hyaluronic\",\n",
    "    \"Ibuprofen\", \"Ondansetron\", \"Mupirocin\", \"Amoxicillin\", \"Doxycycline\",\"Carbachol\", \n",
    "    \"Oral Amoxicillin\",\n",
    "    \n",
    "    # Arabic Medicines\n",
    "    \"Ø­Ù…Ø¶ Ø§Ù„Ø¬Ù„ÙŠÙƒÙˆÙ„ÙŠÙƒ\", \"Ø­Ù…Ø¶ Ø§Ù„Ø³Ø§Ù„ÙŠØ³ÙŠÙ„ÙŠÙƒ\", \"Ø§Ù„Ø¨Ø¨ØªÙŠØ¯Ø§Øª\",\n",
    "    \"Ø£Ù…Ù„Ø§Ø­\", \"Ø¥Ø±ÙŠØ«Ø±ÙˆÙ…Ø§ÙŠØ³ÙŠÙ†\", \"Ø¨Ø§Ø±Ø§Ø³ÙŠØªØ§Ù…ÙˆÙ„\", \"Ø¯ÙˆÙƒØ³ÙŠØ³ÙŠÙƒÙ„ÙŠÙ†\", \"Ù…ÙŠØ¨ÙŠÙ†Ø¯Ø§Ø²ÙˆÙ„\",\n",
    "    \"Ø§Ù„Ø²Ù†Ùƒ\", \"ÙƒØ§Ø±Ø¨ÙˆØ³ÙŠØ³ØªÙŠÙ†\", \"Ø±Ø§Ù†ÙŠØªÙŠØ¯ÙŠÙ†\", \"Ø£ÙˆÙ†Ø¯Ø§Ù†Ø³ÙŠØªØ±ÙˆÙ†\", \"Ù…ÙˆÙ†ØªÙŠÙ„ÙˆÙƒØ§Ø³Øª\",\n",
    "    \"ÙƒÙ„ÙˆØ±Ù‡ÙŠÙƒØ³ÙŠØ¯ÙŠÙ†\", \"Ø¯ÙŠÙƒÙ„ÙˆÙÙŠÙ†Ø§Ùƒ\", \"Ù„ÙŠØ¯ÙˆÙƒØ§ÙŠÙŠÙ†\", \"Ø¨Ø±ÙŠØ¬Ø§Ø¨Ø§Ù„ÙŠÙ†\", \"Ø£Ø²ÙŠØ«Ø±ÙˆÙ…ÙŠØ³ÙŠÙ†\",\n",
    "    \"ÙÙŠØªØ§Ù…ÙŠÙ†\", \"Ù†ÙŠØ§Ø³ÙŠÙ†Ø§Ù…ÙŠØ¯\", \"ØºÙˆØ§ÙŠÙÙŠÙ†ÙŠØ³ÙŠÙ†\", \"ÙØ§Ù„Ø¨Ø±ÙˆØ§Øª\", \"Ø£Ù…ÙˆÙƒØ³ÙŠØ³ÙŠÙ„ÙŠÙ†\",\n",
    "    \"Ù‡ÙŠØ¯Ø±ÙˆÙƒØ³ÙŠØ²ÙŠÙ†\", \"Ø§Ù„Ø¨ÙˆØªÙˆÙƒØ³\", \"Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª\", \"ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ†\", \"Ø¬Ø§Ø¨Ø§Ø¨Ù†ØªÙŠÙ†\",\n",
    "    \"Ø¯ÙŠÙƒØ³Ø§Ù…ÙŠØ«Ø§Ø²ÙˆÙ†\", \"Ø¯ÙŠÙƒØ³ØªØ±ÙˆÙ…ÙŠØ«ÙˆØ±ÙØ§Ù†\", \"Ø³ÙŠØ¨Ø±ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†\", \"Ù„ÙŠÙÙŠØªÙŠØ±Ø§Ø³ÙŠØªØ§Ù…\",\n",
    "    \"Ø§Ù„ÙƒØ§Ù„Ø³ÙŠÙˆÙ…\", \"ØªÙŠÙ…ÙˆÙ„ÙˆÙ„\", \"Ø±ÙˆØ¨ÙŠÙ†ÙŠØ±ÙˆÙ„\", \"Ø§Ù„Ù‡ÙŠØ§Ù„ÙˆØ±ÙˆÙ†ÙŠÙƒ\", \"ÙƒØ§Ø±Ø¨Ø§Ù…Ø§Ø²ÙŠØ¨ÙŠÙ†\",\n",
    "    \"Ù†ÙŠØ³ØªØ§ØªÙŠÙ†\", \"Ø¥ÙŠØ¨ÙˆØ¨Ø±ÙˆÙÙŠÙ†\", \"Ø¨Ø±ÙŠÙ…ÙˆÙ†ÙŠØ¯ÙŠÙ†\", \"Ø£Ù„Ø¨ÙŠÙ†Ø¯Ø§Ø²ÙˆÙ„\", \"Ù‡ÙŠØ¯Ø±ÙˆÙƒÙˆØ±ØªÙŠØ²ÙˆÙ†\",\n",
    "    \"Ø±ÙŠÙØ§Ø³ØªØ¬Ù…ÙŠÙ†\", \"Ù…ÙˆØ¨ÙŠØ±ÙˆØ³ÙŠÙ†\", \"ÙƒÙ„ÙŠÙ†Ø¯Ø§Ù…Ø§ÙŠØ³ÙŠÙ†\", \"Ø£Ø³ÙŠØªÙŠÙ„\", \"Ù‡ÙŠØ¯Ø±ÙˆÙƒÙŠÙ†ÙˆÙ†\",\n",
    "    \"Ø§Ù„Ø¨Ø±ÙˆØ¨ÙŠÙˆØªÙŠÙƒ\", \"Ù…ÙŠØªØ±ÙˆÙ†ÙŠØ¯Ø§Ø²ÙˆÙ„\", \"Ø¯ÙˆÙ†ÙŠØ¨ÙŠØ²ÙŠÙ„\", \"Ø£Ù…Ø§Ù†ØªØ§Ø¯ÙŠÙ†\", \"Ø¨ÙŠÙ„ÙˆÙƒØ§Ø±Ø¨ÙŠÙ†\",\n",
    "    \"Ø³ÙŠÙƒÙ„ÙˆØ³Ø¨ÙˆØ±ÙŠÙ†\", \"ØªØ±ÙŠØªÙŠÙ†ÙˆÙŠÙ†\", \"Ø±ÙŠØªÙŠÙ†ÙˆÙ„\", \"Ø§Ù„Ø­Ø¯ÙŠØ¯\", \"Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†\",\n",
    "    \"ÙÙ„ÙˆØ±ÙˆÙ…ÙŠØ«ÙˆÙ„ÙˆÙ†\", \"Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†\", \"Ø³ÙŠÙÙŠÙƒØ³ÙŠÙ…\", \"Ø³ÙŠÙØ¯ÙŠÙ†ÙŠØ±\", \"ÙƒØ±Ø¨ÙˆÙ†Ø§Øª\", \"ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„\", \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# frequency list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_patterns = [\n",
    "    ## ğŸ”¹ English Frequencies\n",
    "    r\"\\bevery\\s\\d+\\shours?\\b\",            # \"every 6 hours\"\n",
    "    r\"\\bEvery\\shours\\b\",\n",
    "    r\"\\bevery\\s\\d+\\s?-\\s?\\d+\\shours?\\b\",  # \"every 4-6 hours\"\n",
    "    r\"\\bonce\\sdaily\\b\",                   # \"once daily\"\n",
    "    r\"\\btwice\\sdaily\\b\",                  # \"twice daily\"\n",
    "    r\"\\bthree\\stimes\\sa\\sday\\b\",          # \"three times a day\"\n",
    "    r\"\\bfour\\stimes\\sa\\sday\\b\",           # \"four times a day\"\n",
    "    r\"\\b\\d+\\stimes\\sa\\sday\\b\",            # \"5 times a day\"\n",
    "    \n",
    "    ## â³ Time-Based\n",
    "    r\"\\bevery\\sother\\sday\\b\",             # \"every other day\"\n",
    "    r\"\\bevery\\s\\d+\\sdays?\\b\",             # \"every 3 days\"\n",
    "    r\"\\bevery\\s\\d+\\sweeks?\\b\",            # \"every 2 weeks\"\n",
    "    r\"\\bevery\\s\\d+\\smonths?\\b\",           # \"every 6 months\"\n",
    "    \n",
    "    ## ğŸŒ™ Morning/Evening\n",
    "    r\"\\bin\\sthe\\smorning\\b\",              # \"in the morning\"\n",
    "    r\"\\bin\\sthe\\sevening\\b\",              # \"in the evening\"\n",
    "    r\"\\bin\\sthe\\safternoon\\b\",            # \"in the afternoon\"\n",
    "    r\"\\bin\\sthe\\snight\\b\",                # \"in the night\"\n",
    "    r\"\\bdaily\\sat\\snoon\\b\",               # \"daily at noon\"\n",
    "    \n",
    "    ## ğŸ½ Meal-Based\n",
    "    r\"\\bbefore\\smeals?\\b\",                # \"before meals\"\n",
    "    r\"\\bbefore\\sbreakfast?\\b\",            # \"before breakfast\"\n",
    "    r\"\\bafter\\smeals?\\b\",                 # \"after meals\"\n",
    "    r\"\\bafter\\sbreakfast?\\b\",             # \"after breakfast\"\n",
    "    r\"\\bbefore\\sfood\\b\",                  # \"before food\"\n",
    "    r\"\\bafter\\sfood\\b\",                   # \"after food\"\n",
    "    r\"\\bon\\san\\sempty\\sstomach\\b\",        # \"on an empty stomach\"\n",
    "    \n",
    "    ## ğŸŒ™ Sleep\n",
    "    r\"\\bbefore\\sbedtime\\b\",               # \"before bedtime\"\n",
    "    r\"\\bat\\sbedtime\\b\",                   # \"at bedtime\"\n",
    "    r\"\\bbefore\\sgoing\\sto\\sbed\\b\",        # \"before going to bed\"\n",
    "    \n",
    "    ## ğŸ”„ PRN (As Needed)\n",
    "    r\"\\bas\\sneeded\\b\",                    # \"as needed\"\n",
    "    r\"\\bif\\sneeded\\b\",                    # \"if needed\"\n",
    "    r\"\\bwhen\\snecessary\\b\",               # \"when necessary\"\n",
    "    r\"\\bwhen\\srequired\\b\",                # \"when required\"\n",
    "    r\"\\bwhen\\sfeeling\\spain\\b\",           # \"when feeling pain\"\n",
    "    \n",
    "    ## ğŸš‘ Perioperative\n",
    "    r\"\\bbefore\\ssurgery\\b\",               # \"before surgery\"\n",
    "    r\"\\bafter\\ssurgery\\b\",                # \"after surgery\"\n",
    "    r\"\\bbefore\\san\\soperation\\b\",         # \"before an operation\"\n",
    "    r\"\\bafter\\san\\soperation\\b\",          # \"after an operation\"\n",
    "\n",
    "    ## ğŸ”¹ Arabic Frequencies\n",
    "    r\"\\bÙƒÙ„\\s\\d+\\sØ³Ø§Ø¹Ø©\\b\",              # \"ÙƒÙ„ 8 Ø³Ø§Ø¹Ø§Øª\" (every X hours)\n",
    "    r\"\\bÙƒÙ„\\s\\d+\\s?-\\s?\\d+\\sØ³Ø§Ø¹Ø§Øª?\\b\",  # \"ÙƒÙ„ 4-6 Ø³Ø§Ø¹Ø§Øª\" (every X-Y hours)\n",
    "    r\"\\bÙ…Ø±Ø©\\sÙŠÙˆÙ…ÙŠØ§\\b\",                 # \"Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠÙ‹Ø§\" (once daily)\n",
    "    r\"\\bÙ…Ø±Ø©\\sÙƒÙ„\\sÙŠÙˆÙ…\\b\",               # \"Ù…Ø±Ø© ÙƒÙ„ ÙŠÙˆÙ…\" (once per day)\n",
    "    r\"\\bÙ…Ø±Ø©\\sÙŠÙˆÙ…ÙŠØ§\\b\",                 # \"Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§\" (once per day)\n",
    "    r\"\\bÙ…Ø±Ø©\\sØ£Ø³Ø¨ÙˆØ¹ÙŠØ§\\b\",               # \"Ù…Ø±Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠÙ‹Ø§\" (once weekly)\n",
    "    r\"\\bÙ…Ø±Ø©\\sÙƒÙ„\\sØ£Ø³Ø¨ÙˆØ¹\\b\",             # \"Ù…Ø±Ø© ÙƒÙ„ Ø£Ø³Ø¨ÙˆØ¹\" (once per week)\n",
    "    r\"\\bÙ…Ø±Ø©\\sØ´Ù‡Ø±ÙŠØ§\\b\",                 # \"Ù…Ø±Ø© Ø´Ù‡Ø±ÙŠÙ‹Ø§\" (once monthly)\n",
    "    r\"\\bÙ…Ø±Ø©\\sÙƒÙ„\\sØ´Ù‡Ø±\\b\",               # \"Ù…Ø±Ø© ÙƒÙ„ Ø´Ù‡Ø±\" (once per month)\n",
    "    r\"\\bÙ…Ø±ØªÙŠÙ†\\sÙŠÙˆÙ…ÙŠØ§\\b\",               # \"Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠÙ‹Ø§\" (twice daily)\n",
    "    r\"\\b\\d+\\sÙ…Ø±Ø§Øª?\\sÙŠÙˆÙ…ÙŠØ§\\b\",          # \"3 Ù…Ø±Ø§Øª ÙŠÙˆÙ…ÙŠÙ‹Ø§\" (multiple times daily)\n",
    "    \n",
    "    ## â³ Time-Based\n",
    "    r\"\\bÙƒÙ„\\sÙŠÙˆÙ…ÙŠÙ†\\b\",                   # \"ÙƒÙ„ ÙŠÙˆÙ…ÙŠÙ†\" (every other day)\n",
    "    r\"\\bÙƒÙ„\\s\\d+\\sØ£ÙŠØ§Ù…\\b\",               # \"ÙƒÙ„ 3 Ø£ÙŠØ§Ù…\" (every X days)\n",
    "    r\"\\bÙƒÙ„\\s\\d+\\sØ£Ø³Ø§Ø¨ÙŠØ¹\\b\",             # \"ÙƒÙ„ 2 Ø£Ø³Ø§Ø¨ÙŠØ¹\" (every X weeks)\n",
    "    r\"\\bÙƒÙ„\\s\\d+\\sØ´Ù‡ÙˆØ±\\b\",               # \"ÙƒÙ„ 6 Ø´Ù‡ÙˆØ±\" (every X months)\n",
    "\n",
    "   ## Arabic (Word-Based Numbers)\n",
    "    r\"\\bÙƒÙ„\\sØ³Ø§Ø¹Ø©\\b\",                     # \"ÙƒÙ„ ÙˆØ§Ø­Ø¯Ø© Ø³Ø§Ø¹Ø©\" (every one hour)\n",
    "    r\"\\bÙƒÙ„\\sØ³Ø§Ø¹ØªÙŠÙ†\\b\",                   # \"ÙƒÙ„ Ø§Ø«Ù†ØªÙŠÙ† Ø³Ø§Ø¹Ø©\" (every two hours)\n",
    "    r\"\\bÙƒÙ„\\sØ«Ù„Ø§Ø«\\sØ³Ø§Ø¹Ø§Øª\\b\",               # \"ÙƒÙ„ Ø«Ù„Ø§Ø« Ø³Ø§Ø¹Ø§Øª\" (every three hours)\n",
    "    r\"\\bÙƒÙ„\\sØ£Ø±Ø¨Ø¹\\sØ³Ø§Ø¹Ø§Øª\\b\",              # \"ÙƒÙ„ Ø£Ø±Ø¨Ø¹ Ø³Ø§Ø¹Ø§Øª\" (every four hours)\n",
    "    r\"\\bÙƒÙ„\\sØ®Ù…Ø³\\sØ³Ø§Ø¹Ø§Øª\\b\",               # \"ÙƒÙ„ Ø®Ù…Ø³ Ø³Ø§Ø¹Ø§Øª\" (every five hours)\n",
    "    r\"\\bÙƒÙ„\\sØ³Øª\\sØ³Ø§Ø¹Ø§Øª\\b\",                # \"ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª\" (every six hours)\n",
    "    r\"\\bÙƒÙ„\\sØ³Ø¨Ø¹\\sØ³Ø§Ø¹Ø§Øª\\b\",               # \"ÙƒÙ„ Ø³Ø¨Ø¹ Ø³Ø§Ø¹Ø§Øª\" (every seven hours)\n",
    "    r\"\\bÙƒÙ„\\sØ«Ù…Ø§Ù†ÙŠ\\sØ³Ø§Ø¹Ø§Øª\\b\",             # \"ÙƒÙ„ Ø«Ù…Ø§Ù†ÙŠ Ø³Ø§Ø¹Ø§Øª\" (every eight hours)\n",
    "    r\"\\bÙƒÙ„\\sØªØ³Ø¹\\sØ³Ø§Ø¹Ø§Øª\\b\",               # \"ÙƒÙ„ ØªØ³Ø¹ Ø³Ø§Ø¹Ø§Øª\" (every nine hours)\n",
    "    r\"\\bÙƒÙ„\\sØ¹Ø´Ø±\\sØ³Ø§Ø¹Ø§Øª\\b\",               # \"ÙƒÙ„ Ø¹Ø´Ø± Ø³Ø§Ø¹Ø§Øª\" (every ten hours)\n",
    "    r\"\\bÙƒÙ„\\sØ¥Ø­Ø¯Ù‰\\sØ¹Ø´Ø±Ø©\\sØ³Ø§Ø¹Ø©\\b\",         # \"ÙƒÙ„ Ø¥Ø­Ø¯Ù‰ Ø¹Ø´Ø±Ø© Ø³Ø§Ø¹Ø©\" (every 11 hours)\n",
    "    r\"\\bÙƒÙ„\\sØ§Ø«Ù†ØªÙŠ\\sØ¹Ø´Ø±Ø©\\sØ³Ø§Ø¹Ø©\\b\",        # \"ÙƒÙ„ Ø§Ø«Ù†ØªÙŠ Ø¹Ø´Ø±Ø© Ø³Ø§Ø¹Ø©\" (every 12 hours)\n",
    "    \n",
    "    ## ğŸŒ™ Morning/Evening\n",
    "    r\"\\bÙÙŠ\\sØ§Ù„ØµØ¨Ø§Ø­\\b\",                  # \"ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­\" (in the morning)\n",
    "    r\"\\bÙÙŠ\\sØ§Ù„Ù…Ø³Ø§Ø¡\\b\",                  # \"ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¡\" (in the evening)\n",
    "    r\"\\bÙÙŠ\\sØ§Ù„Ø¸Ù‡ÙŠØ±Ø©\\b\",                 # \"ÙÙŠ Ø§Ù„Ø¸Ù‡ÙŠØ±Ø©\" (at noon)\n",
    "    r\"\\bÙÙŠ\\sØ§Ù„Ù„ÙŠÙ„\\b\",                   # \"ÙÙŠ Ø§Ù„Ù„ÙŠÙ„\" (at night)\n",
    "    \n",
    "    ## ğŸ½ Meal-Based\n",
    "    r\"\\bÙ‚Ø¨Ù„\\sØ§Ù„Ø£ÙƒÙ„\\b\",                  # \"Ù‚Ø¨Ù„ Ø§Ù„Ø£ÙƒÙ„\" (before meals)\n",
    "    r\"\\bØ¨Ø¹Ø¯\\sØ§Ù„Ø£ÙƒÙ„\\b\",                  # \"Ø¨Ø¹Ø¯ Ø§Ù„Ø£ÙƒÙ„\" (after meals)\n",
    "    r\"\\bÙ‚Ø¨Ù„\\sØ§Ù„Ø·Ø¹Ø§Ù…\\b\",                 # \"Ù‚Ø¨Ù„ Ø§Ù„Ø·Ø¹Ø§Ù…\" (before food)\n",
    "    r\"\\bØ¨Ø¹Ø¯\\sØ§Ù„Ø·Ø¹Ø§Ù…\\b\",                 # \"Ø¨Ø¹Ø¯ Ø§Ù„Ø·Ø¹Ø§Ù…\" (after food)\n",
    "    r\"\\bØ¹Ù„Ù‰\\sÙ…Ø¹Ø¯Ø©\\sÙØ§Ø±ØºØ©\\b\",            # \"Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ø© ÙØ§Ø±ØºØ©\" (on an empty stomach)\n",
    "    r\"\\bØ¹Ù„Ù‰\\sØ§Ù„Ø±ÙŠÙ‚\\b\",                   # \"Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙŠÙ‚\" (fasting)\n",
    "    \n",
    "    ## ğŸŒ™ Sleep\n",
    "    r\"\\bÙ‚Ø¨Ù„\\sØ§Ù„Ù†ÙˆÙ…\\b\",                 # \"Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\" (before sleep)\n",
    "    r\"\\bØ¹Ù†Ø¯\\sØ§Ù„Ù†ÙˆÙ…\\b\",                 # \"Ø¹Ù†Ø¯ Ø§Ù„Ù†ÙˆÙ…\" (at bedtime)\n",
    "    \n",
    "    ## ğŸ”„ PRN (As Needed)\n",
    "    r\"\\bØ¹Ù†Ø¯\\sØ§Ù„Ù„Ø²ÙˆÙ…\\b\",                # \"Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…\" (as needed)\n",
    "    r\"\\bØ­Ø³Ø¨\\sØ§Ù„Ø­Ø§Ø¬Ø©\\b\",                # \"Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\" (as required)\n",
    "    r\"\\bØ¥Ø°Ø§\\sØ§Ø³ØªØ¯Ø¹Øª\\sØ§Ù„Ø­Ø§Ø¬Ø©\\b\",         # \"Ø¥Ø°Ø§ Ø§Ø³ØªØ¯Ø¹Øª Ø§Ù„Ø­Ø§Ø¬Ø©\" (if necessary)\n",
    "    r\"\\bØ¹Ù†Ø¯\\sØ§Ù„Ø´Ø¹ÙˆØ±\\sØ¨Ø§Ù„Ø£Ù„Ù…\\b\",          # \"Ø¹Ù†Ø¯ Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø£Ù„Ù…\" (when feeling pain)\n",
    "    \n",
    "    ## ğŸš‘ Perioperative\n",
    "    r\"\\bÙ‚Ø¨Ù„\\sØ§Ù„Ø¹Ù…Ù„ÙŠØ©\\b\",              # \"Ù‚Ø¨Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©\" (before surgery)\n",
    "    r\"\\bØ¨Ø¹Ø¯\\sØ§Ù„Ø¹Ù…Ù„ÙŠØ©\\b\",              # \"Ø¨Ø¹Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©\" (after surgery)\n",
    "    r\"\\bÙ‚Ø¨Ù„\\sØ§Ù„ØªØ¯Ø®Ù„\\sØ§Ù„Ø¬Ø±Ø§Ø­ÙŠ\\b\",     # \"Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ø¬Ø±Ø§Ø­ÙŠ\" (before an operation)\n",
    "    r\"\\bØ¨Ø¹Ø¯\\sØ§Ù„ØªØ¯Ø®Ù„\\sØ§Ù„Ø¬Ø±Ø§Ø­ÙŠ\\b\",     # \"Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ø¬Ø±Ø§Ø­ÙŠ\" (after an operation)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Assuming CSV)\n",
    "df = pd.read_excel(\"Train.xlsx\")\n",
    "\n",
    "# Specify the column containing extracted prescription text\n",
    "target_column = \"Prescription\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0                                                                                Carbachol Every 8 hours\n",
       "1         Ofloxacin ÙƒÙ„ Ø³Øª  Ø³Ø§Ø¹Ø§Øª , Brimonidine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª , ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Latanoprost Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\n",
       "2                                                                           Brimonidine Before breakfast\n",
       "3          Carbachol Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Tobramycin Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Pilocarpine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª , Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\n",
       "4                                                      Fluorometholone As needed , Latanoprost As needed\n",
       "                                                     ...                                                \n",
       "618    Doxycycline Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹ , ÙƒÙ„ÙŠÙ†Ø¯Ø§Ù…Ø§ÙŠØ³ÙŠÙ† Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ… , Ù…ÙˆØ¨ÙŠØ±ÙˆØ³ÙŠÙ† Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠØ§Ù‹ , ÙƒÙ„ÙˆØ±Ù‡ÙŠÙƒØ³ÙŠØ¯ÙŠÙ† Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\n",
       "619                                                                              Ø¯ÙŠÙƒÙ„ÙˆÙÙŠÙ†Ø§Ùƒ Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠØ§Ù‹\n",
       "620                                                                                  Nystatin Ø¨Ø¹Ø¯ Ø§Ù„ØºØ¯Ø§Ø¡\n",
       "621                       Azithromycin Ù‚Ø¨Ù„ Ø§Ù„ÙØ·Ø§Ø± , Ø³ÙŠØ¨Ø±ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ† Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Metronidazole Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\n",
       "622                      Mupirocin Once daily , Hydrocortisone Twice daily , Ciprofloxacin Every 6 hours\n",
       "Name: Prescription, Length: 623, dtype: object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target_column].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_medicine_and_frequency(text):\n",
    "    \"\"\"\n",
    "    Extracts medicine names and dosage frequency while maintaining the original order.\n",
    "    Returns a list of dictionaries [{medicine: \"name\", frequency: \"value\"}].\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "    \n",
    "    structured_output = []\n",
    "    \n",
    "    # Combine medicine names and frequency patterns into a single regex pattern\n",
    "    combined_pattern = r\"|\".join(\n",
    "        [re.escape(med) for med in medicine_list] + frequency_patterns\n",
    "    )\n",
    "\n",
    "    # Find all matches in the order they appear in the text\n",
    "    matches = re.findall(combined_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    current_medicine = None  # Track last detected medicine\n",
    "    \n",
    "    for match in matches:\n",
    "        if match in medicine_list:  \n",
    "            current_medicine = match  # Update the latest detected medicine\n",
    "            structured_output.append({\"medicine\": match, \"frequency\": \"\"})  # Default frequency\n",
    "        \n",
    "        elif current_medicine:  \n",
    "            # Assign the frequency to the last detected medicine **only if it does not have one yet**\n",
    "            if structured_output[-1][\"frequency\"] == \"\":\n",
    "                structured_output[-1][\"frequency\"] = match  \n",
    "            else:\n",
    "                # If previous medicine already has a frequency, store this frequency separately\n",
    "                structured_output.append({\"medicine\": current_medicine, \"frequency\": match})\n",
    "    \n",
    "    return structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                             [{'medicine': 'Carbachol', 'frequency': 'Every 8 hours'}]\n",
       "1                [{'medicine': 'Ofloxacin', 'frequency': ''}, {'medicine': 'Brimonidine', 'frequency': ''}, {'medicine': 'ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„', 'frequency': 'Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…'}, {'medicine': 'Latanoprost', 'frequency': 'Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§'}]\n",
       "2                                                                                                                                                        [{'medicine': 'Brimonidine', 'frequency': 'Before breakfast'}]\n",
       "3    [{'medicine': 'Carbachol', 'frequency': 'Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…'}, {'medicine': 'Tobramycin', 'frequency': 'Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…'}, {'medicine': 'Pilocarpine', 'frequency': ''}, {'medicine': 'Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª', 'frequency': 'Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…'}]\n",
       "4                                                                                                    [{'medicine': 'Fluorometholone', 'frequency': 'As needed'}, {'medicine': 'Latanoprost', 'frequency': 'As needed'}]\n",
       "Name: structured_prescriptions, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply extraction function to dataset\n",
    "df[\"structured_prescriptions\"] = df[target_column].apply(extract_medicine_and_frequency)\n",
    "\n",
    "# Convert the entire dataset into a list of lists of dictionaries\n",
    "structured_output = df[\"structured_prescriptions\"].tolist()\n",
    "\n",
    "# Print the final structured output\n",
    "df[\"structured_prescriptions\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_medicine_and_frequency(text):\n",
    "    if not isinstance(text, str):  # Ensure text is a string\n",
    "        return []\n",
    "    \n",
    "    structured_output = []\n",
    "    \n",
    "    # Combine medicine names and frequency patterns into a single regex pattern\n",
    "    combined_pattern = r\"|\".join(\n",
    "        [re.escape(med) for med in medicine_list] + frequency_patterns\n",
    "    )\n",
    "\n",
    "    # Find all matches in the order they appear in the text\n",
    "    matches = re.findall(combined_pattern, text, re.IGNORECASE)\n",
    "    \n",
    "    current_medicine = None  # Track last detected medicine\n",
    "    \n",
    "    for match in matches:\n",
    "        if match in medicine_list:  \n",
    "            current_medicine = match  # Update the latest detected medicine\n",
    "            structured_output.append({\"medicine\": match, \"frequency\": \"\"})  # Default frequency\n",
    "        \n",
    "        elif current_medicine:\n",
    "            # Assign the frequency to the last detected medicine **only if it does not have one yet**\n",
    "            if structured_output[-1][\"frequency\"] == \"\":\n",
    "                structured_output[-1][\"frequency\"] = match  \n",
    "            else:\n",
    "                # If previous medicine already has a frequency, store this frequency separately\n",
    "                structured_output.append({\"medicine\": current_medicine, \"frequency\": match})\n",
    "    \n",
    "    return structured_output\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    # Normalize the text to decompose characters into base characters and diacritics\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Filter out combining characters (diacritics)\n",
    "    cleaned_text = ''.join(\n",
    "        char for char in normalized_text\n",
    "        if not unicodedata.combining(char)\n",
    "    )\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'medicine': 'Ofloxacin', 'frequency': 'ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª'},\n",
       " {'medicine': 'Brimonidine', 'frequency': ''},\n",
       " {'medicine': 'ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„', 'frequency': 'Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…'},\n",
       " {'medicine': 'Latanoprost', 'frequency': 'Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Ofloxacin ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª , Brimonidine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª , ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Latanoprost Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\"\n",
    "# text = \"Carbachol Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Tobramycin Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ… , Pilocarpine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª , Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\"\n",
    "# text = \"Doxycycline Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹ , ÙƒÙ„ÙŠÙ†Ø¯Ø§Ù…Ø§ÙŠØ³ÙŠÙ† Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ… , Ù…ÙˆØ¨ÙŠØ±ÙˆØ³ÙŠÙ† Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠØ§Ù‹ , ÙƒÙ„ÙˆØ±Ù‡ÙŠÙƒØ³ÙŠØ¯ÙŠÙ† Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\"\n",
    "\n",
    "text = remove_diacritics(text)\n",
    "result = extract_medicine_and_frequency(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_FREQUENCY = \"Every 6 hours\"\n",
    "\n",
    "def extract_medicine_and_frequency(sentence, medicine_list, frequency_patterns):\n",
    "    extracted_medicines = []\n",
    "    extracted_frequencies = []\n",
    "    medicine_positions = {}\n",
    "    frequency_positions = {}\n",
    "\n",
    "    # Step 1: Extract medicines and their positions\n",
    "    for med in medicine_list:\n",
    "        match = re.search(rf\"\\b{re.escape(med)}\\b\", sentence)\n",
    "        if match:\n",
    "            extracted_medicines.append(med)\n",
    "            medicine_positions[med] = match.start()\n",
    "\n",
    "    # Step 2: Extract frequencies and their positions\n",
    "    for pattern in frequency_patterns:\n",
    "        pattern_re = re.compile(pattern)\n",
    "        matches = pattern_re.finditer(sentence)\n",
    "        for match in matches:\n",
    "            extracted_frequencies.append(match.group())\n",
    "            frequency_positions[match.group()] = match.start()\n",
    "\n",
    "    # Step 3: Sort medicines and frequencies by their position\n",
    "    sorted_medicines = sorted(medicine_positions.items(), key=lambda x: x[1])\n",
    "    sorted_frequencies = sorted(frequency_positions.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Step 4: Pair medicines with frequencies\n",
    "    extracted_results = []\n",
    "    freq_index = 0\n",
    "    last_known_frequency = None\n",
    "    \n",
    "    for i, (med, med_pos) in enumerate(sorted_medicines):\n",
    "        extracted_frequency = \"Unknown\"\n",
    "        next_med_pos = sorted_medicines[i + 1][1] if i + 1 < len(sorted_medicines) else len(sentence)\n",
    "\n",
    "        # Check if there are words between this medicine and the next\n",
    "        words_between = sentence[med_pos + len(med):next_med_pos].strip()\n",
    "        if words_between and len(words_between.split()) <= 3:\n",
    "            extracted_frequency = words_between\n",
    "        else:\n",
    "            # Try to match frequency after the medicine (if ordered correctly)\n",
    "            while freq_index < len(sorted_frequencies):\n",
    "                freq, freq_pos = sorted_frequencies[freq_index]\n",
    "                if freq_pos > med_pos and freq_pos < next_med_pos:\n",
    "                    extracted_frequency = freq\n",
    "                    last_known_frequency = freq\n",
    "                    freq_index += 1\n",
    "                    break\n",
    "                freq_index += 1\n",
    "\n",
    "            # If no direct match, use last known frequency\n",
    "            if extracted_frequency == \"Unknown\" and last_known_frequency:\n",
    "                extracted_frequency = last_known_frequency\n",
    "\n",
    "            # If still unknown, use default\n",
    "            if extracted_frequency == \"Unknown\":\n",
    "                extracted_frequency = DEFAULT_FREQUENCY\n",
    "\n",
    "        extracted_results.append((med, extracted_frequency))\n",
    "    \n",
    "    return extracted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paracetamol hours Every Oral Amoxicillin other lunch After Erythromycin Nystatin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Paracetamol', 'hours Every'),\n",
       " ('Oral Amoxicillin', 'Every 6 hours'),\n",
       " ('Amoxicillin', 'other lunch After'),\n",
       " ('Erythromycin', 'Every 6 hours'),\n",
       " ('Nystatin', 'Every 6 hours')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"Doxycycline Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹ , ÙƒÙ„ÙŠÙ†Ø¯Ø§Ù…Ø§ÙŠØ³ÙŠÙ† Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ… , Ù…ÙˆØ¨ÙŠØ±ÙˆØ³ÙŠÙ† Ù…Ø±ØªÙŠÙ† ÙŠÙˆÙ…ÙŠØ§Ù‹ , ÙƒÙ„ÙˆØ±Ù‡ÙŠÙƒØ³ÙŠØ¯ÙŠÙ† Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\"\n",
    "text = \"Paracetamol hours Every Oral Amoxicillin other lunch After Erythromycin Nystatin\"\n",
    "\n",
    "print(text)\n",
    "text = remove_diacritics(text)\n",
    "result = extract_medicine_and_frequency(text, medicine_list, frequency_patterns)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing prescriptions using regex-based extraction...\n",
      "\n",
      "Prescription 1:\n",
      "Text: Carbachol Every 8 hours\n",
      "Medication Schedule:\n",
      "  - Medication: Carbachol, Frequency: Every 8 hours (Confidence: 0.99)\n",
      "\n",
      "Prescription 2:\n",
      "Text: Ofloxacin ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª, Brimonidine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Latanoprost Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\n",
      "Medication Schedule:\n",
      "  - Medication: Ofloxacin, Frequency: every 6 hours (Confidence: 0.99)\n",
      "  - Medication: Brimonidine, Frequency: every 6 hours (Confidence: 0.99)\n",
      "  - Medication: Carbachol, Frequency: as needed (Confidence: 0.99)\n",
      "  - Medication: Latanoprost, Frequency: once daily (Confidence: 0.99)\n",
      "\n",
      "Prescription 3:\n",
      "Text: Brimonidine Before breakfast\n",
      "Medication Schedule:\n",
      "  - Medication: Brimonidine, Frequency: Before breakfast (Confidence: 0.99)\n",
      "\n",
      "Prescription 4:\n",
      "Text: Carbachol Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Tobramycin Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Pilocarpine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\n",
      "Medication Schedule:\n",
      "  - Medication: Carbachol, Frequency: as needed (Confidence: 0.99)\n",
      "  - Medication: Tobramycin, Frequency: as needed (Confidence: 0.99)\n",
      "  - Medication: Pilocarpine, Frequency: every 6 hours (Confidence: 0.99)\n",
      "  - Medication: Latanoprost, Frequency: before sleep (Confidence: 0.99)\n",
      "\n",
      "Prescription 5:\n",
      "Text: Fluorometholone As needed, Latanoprost As needed\n",
      "Medication Schedule:\n",
      "  - Medication: Fluorometholone, Frequency: As needed (Confidence: 0.99)\n",
      "  - Medication: Latanoprost, Frequency: As needed (Confidence: 0.99)\n",
      "\n",
      "Prescription 6:\n",
      "Text: Brimonidine Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, Dorzolamide Ø¨Ø¹Ø¯ Ø§Ù„ØºØ¯Ø§Ø¡\n",
      "Medication Schedule:\n",
      "  - Medication: Brimonidine, Frequency: before sleep (Confidence: 0.99)\n",
      "  - Medication: Dorzolamide, Frequency: after lunch (Confidence: 0.99)\n",
      "\n",
      "Prescription 7:\n",
      "Text: Ø¯ÙˆØ±Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯ Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, ØªÙŠÙ…ÙˆÙ„ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Ù‚Ø·Ø±Ø§Øª Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ† ÙƒÙ„ Ù¨ Ø³Ø§Ø¹Ø§Øª, Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ† ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\n",
      "Medication Schedule:\n",
      "  - Medication: Dorzolamide, Frequency: before sleep (Confidence: 0.99)\n",
      "  - Medication: Timolol, Frequency: as needed (Confidence: 0.99)\n",
      "  - Medication: Prednisolone, Frequency: every 8 hours (Confidence: 0.99)\n",
      "  - Medication: Ofloxacin, Frequency: every 6 hours (Confidence: 0.99)\n",
      "\n",
      "Prescription 8:\n",
      "Text: Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ† Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹, ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\n",
      "Medication Schedule:\n",
      "  - Medication: Latanoprost, Frequency: before sleep (Confidence: 0.99)\n",
      "  - Medication: Tobramycin, Frequency: once daily (Confidence: 0.99)\n",
      "  - Medication: Carbachol, Frequency: every 6 hours (Confidence: 0.99)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from datasets import Dataset\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "import numpy as np\n",
    "\n",
    "class MedicalPrescriptionNER:\n",
    "    def __init__(self, model_name=\"distilbert-base-multilingual-cased\"):\n",
    "        \"\"\"\n",
    "        Initialize the NER system for medical prescriptions\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.ner_pipeline = None\n",
    "        \n",
    "        # For custom training\n",
    "        self.labels = [\"O\", \"B-MEDICATION\", \"I-MEDICATION\", \"B-FREQUENCY\", \"I-FREQUENCY\"]\n",
    "        self.id2label = {i: label for i, label in enumerate(self.labels)}\n",
    "        self.label2id = {label: i for i, label in enumerate(self.labels)}\n",
    "        \n",
    "        # Arabic frequency phrases and their standardized mappings\n",
    "        self.arabic_frequencies = {\n",
    "            \"ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª\": \"every 6 hours\",\n",
    "            \"ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\": \"every 6 hours\",\n",
    "            \"Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…\": \"as needed\",\n",
    "            \"Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\": \"once daily\",\n",
    "            \"Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\": \"before sleep\",\n",
    "            \"Ù‚Ø¨Ù„ Ø§Ù„Ø¥ÙØ·Ø§Ø±\": \"before breakfast\",\n",
    "            \"Ø¨Ø¹Ø¯ Ø§Ù„ØºØ¯Ø§Ø¡\": \"after lunch\",\n",
    "            \"ÙƒÙ„ Ù¨ Ø³Ø§Ø¹Ø§Øª\": \"every 8 hours\"\n",
    "        }\n",
    "        \n",
    "        # English medication names and their Arabic transliterations\n",
    "        self.medication_mappings = {\n",
    "            \"ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„\": \"Carbachol\",\n",
    "            \"Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ†\": \"Ofloxacin\",\n",
    "            \"Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª\": \"Latanoprost\",\n",
    "            \"Ø¨Ø±ÙŠÙ…ÙˆÙ†ÙŠØ¯ÙŠÙ†\": \"Brimonidine\",\n",
    "            \"ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ†\": \"Tobramycin\",\n",
    "            \"Ø¨ÙŠÙ„ÙˆÙƒØ§Ø±Ø¨ÙŠÙ†\": \"Pilocarpine\",\n",
    "            \"ÙÙ„ÙˆØ±ÙˆÙ…ÙŠØ«ÙˆÙ„ÙˆÙ†\": \"Fluorometholone\",\n",
    "            \"Ø¯ÙˆØ±Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯\": \"Dorzolamide\",\n",
    "            \"ØªÙŠÙ…ÙˆÙ„ÙˆÙ„\": \"Timolol\",\n",
    "            \"Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ†\": \"Prednisolone\"\n",
    "        }\n",
    "        \n",
    "        # Initialize pattern matchers\n",
    "        self._initialize_patterns()\n",
    "    \n",
    "    def _initialize_patterns(self):\n",
    "        \"\"\"Initialize regex patterns for medications and frequencies\"\"\"\n",
    "        # Common medication patterns\n",
    "        common_medications = [\n",
    "            \"Carbachol\", \"Ofloxacin\", \"Brimonidine\", \"Latanoprost\", \"Tobramycin\",\n",
    "            \"Pilocarpine\", \"Fluorometholone\", \"Dorzolamide\", \"Timolol\", \"Prednisolone\"\n",
    "        ]\n",
    "        \n",
    "        # Build pattern for medication names (case insensitive)\n",
    "        self.medication_pattern = re.compile(\n",
    "            r'\\b(' + '|'.join(common_medications) + r')\\b', \n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Common frequency patterns in English\n",
    "        self.eng_frequency_pattern = re.compile(\n",
    "            r'\\b(every\\s+\\d+\\s+hours?|'\n",
    "            r'once daily|twice daily|'\n",
    "            r'before (breakfast|lunch|dinner|sleep|bed)|'\n",
    "            r'after (breakfast|lunch|dinner)|'\n",
    "            r'as needed|daily|weekly|monthly|'\n",
    "            r'\\d+ times? (daily|a day))\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "        # Arabic frequency patterns (needs simplified regex for demo)\n",
    "        self.arabic_frequency_pattern = re.compile(\n",
    "            '|'.join(map(re.escape, self.arabic_frequencies.keys()))\n",
    "        )\n",
    "    \n",
    "    def load_pretrained_model(self):\n",
    "        \"\"\"Load a pretrained multilingual NER model\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.model_name, \n",
    "            num_labels=len(self.labels),\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id\n",
    "        )\n",
    "        self.ner_pipeline = pipeline(\n",
    "            \"ner\", \n",
    "            model=self.model, \n",
    "            tokenizer=self.tokenizer,\n",
    "            aggregation_strategy=\"simple\"  # Merge subwords\n",
    "        )\n",
    "        \n",
    "        print(f\"Loaded pretrained model: {self.model_name}\")\n",
    "        \n",
    "    def prepare_training_data(self, prescriptions):\n",
    "        \"\"\"\n",
    "        Prepare training data from prescription list using pattern matching\n",
    "        Returns data in format suitable for spaCy training\n",
    "        \"\"\"\n",
    "        training_data = []\n",
    "        \n",
    "        for text in prescriptions:\n",
    "            doc = {\"text\": text, \"entities\": []}\n",
    "            \n",
    "            # Find medications using regex\n",
    "            for med_match in self.medication_pattern.finditer(text):\n",
    "                doc[\"entities\"].append({\n",
    "                    \"start\": med_match.start(),\n",
    "                    \"end\": med_match.end(),\n",
    "                    \"label\": \"MEDICATION\"\n",
    "                })\n",
    "            \n",
    "            # Find Arabic medication names\n",
    "            for ar_med, en_med in self.medication_mappings.items():\n",
    "                for match in re.finditer(re.escape(ar_med), text):\n",
    "                    doc[\"entities\"].append({\n",
    "                        \"start\": match.start(),\n",
    "                        \"end\": match.end(),\n",
    "                        \"label\": \"MEDICATION\"\n",
    "                    })\n",
    "            \n",
    "            # Find English frequencies\n",
    "            for freq_match in self.eng_frequency_pattern.finditer(text):\n",
    "                doc[\"entities\"].append({\n",
    "                    \"start\": freq_match.start(),\n",
    "                    \"end\": freq_match.end(),\n",
    "                    \"label\": \"FREQUENCY\"\n",
    "                })\n",
    "            \n",
    "            # Find Arabic frequencies\n",
    "            for ar_freq_match in self.arabic_frequency_pattern.finditer(text):\n",
    "                doc[\"entities\"].append({\n",
    "                    \"start\": ar_freq_match.start(),\n",
    "                    \"end\": ar_freq_match.end(),\n",
    "                    \"label\": \"FREQUENCY\"\n",
    "                })\n",
    "            \n",
    "            training_data.append(doc)\n",
    "        \n",
    "        return training_data\n",
    "    \n",
    "    def prepare_spacy_data(self, training_data):\n",
    "        \"\"\"Convert training data to spaCy format\"\"\"\n",
    "        nlp = spacy.blank(\"xx\")  # multilingual blank model\n",
    "        db = DocBin()\n",
    "        \n",
    "        for example in training_data:\n",
    "            text = example[\"text\"]\n",
    "            doc = nlp.make_doc(text)\n",
    "            \n",
    "            # Sort entities by start position\n",
    "            entities = sorted(example[\"entities\"], key=lambda e: e[\"start\"])\n",
    "            \n",
    "            # Create span objects\n",
    "            spans = [doc.char_span(e[\"start\"], e[\"end\"], label=e[\"label\"]) \n",
    "                     for e in entities]\n",
    "            spans = [span for span in spans if span is not None]\n",
    "            \n",
    "            # Filter overlapping spans\n",
    "            filtered_spans = filter_spans(spans)\n",
    "            \n",
    "            # Add spans to document\n",
    "            doc.spans[\"entities\"] = filtered_spans\n",
    "            db.add(doc)\n",
    "        \n",
    "        return db\n",
    "    \n",
    "    def train_spacy_model(self, training_data, output_dir=\"./med_ner_model\"):\n",
    "        \"\"\"Train a spaCy model with the prepared data\"\"\"\n",
    "        db = self.prepare_spacy_data(training_data)\n",
    "        \n",
    "        # Save converted training data\n",
    "        db.to_disk(\"./train_data.spacy\")\n",
    "        \n",
    "        # Command to run in shell (not executed here):\n",
    "        print(\"Run the following command to train the model:\")\n",
    "        print(f\"python -m spacy train config.cfg --output {output_dir} --paths.train train_data.spacy --paths.dev train_data.spacy\")\n",
    "    \n",
    "    def train_transformer_model(self, training_data, output_dir=\"./med_ner_transformer\"):\n",
    "        \"\"\"Train a transformer model with the prepared data\"\"\"\n",
    "        # Convert to IOB format for transformer training\n",
    "        tokenized_inputs = []\n",
    "        tags = []\n",
    "        \n",
    "        for example in training_data:\n",
    "            text = example[\"text\"]\n",
    "            tokens = []\n",
    "            labels = [\"O\"] * len(text.split())  # Initialize all as Outside\n",
    "            \n",
    "            # Process each character position\n",
    "            for i, char in enumerate(text):\n",
    "                # Check if this position starts an entity\n",
    "                for entity in example[\"entities\"]:\n",
    "                    if i == entity[\"start\"]:\n",
    "                        entity_tokens = text[entity[\"start\"]:entity[\"end\"]].split()\n",
    "                        entity_idx = len(tokens)\n",
    "                        for j, token in enumerate(entity_tokens):\n",
    "                            if j == 0:\n",
    "                                labels[entity_idx + j] = f\"B-{entity['label']}\"\n",
    "                            else:\n",
    "                                labels[entity_idx + j] = f\"I-{entity['label']}\"\n",
    "                \n",
    "                # Add token if it's a word boundary\n",
    "                if char.isspace() or i == 0:\n",
    "                    if i > 0 and text[i-1].isspace():\n",
    "                        continue\n",
    "                    word = \"\"\n",
    "                    j = i\n",
    "                    while j < len(text) and not text[j].isspace():\n",
    "                        word += text[j]\n",
    "                        j += 1\n",
    "                    if word:\n",
    "                        tokens.append(word)\n",
    "            \n",
    "            tokenized_inputs.append(tokens)\n",
    "            tags.append(labels[:len(tokens)])  # Ensure labels match tokens\n",
    "        \n",
    "        # Convert to HuggingFace dataset format\n",
    "        dataset_dict = {\n",
    "            \"tokens\": tokenized_inputs,\n",
    "            \"ner_tags\": tags\n",
    "        }\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        \n",
    "        # Print training info\n",
    "        print(f\"Created training dataset with {len(dataset)} examples\")\n",
    "        print(\"Sample tokens:\", dataset[0][\"tokens\"])\n",
    "        print(\"Sample tags:\", dataset[0][\"ner_tags\"])\n",
    "        \n",
    "        # Code to train the transformer model (not executed here)\n",
    "        print(\"\\nTo train the transformer model, use HuggingFace Trainer...\")\n",
    "    \n",
    "    def extract_entities_with_regex(self, text):\n",
    "        \"\"\"Extract medications and frequencies using regex patterns\"\"\"\n",
    "        entities = []\n",
    "        \n",
    "        # Extract English medications\n",
    "        for match in self.medication_pattern.finditer(text):\n",
    "            entities.append({\n",
    "                \"entity\": \"MEDICATION\",\n",
    "                \"value\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        \n",
    "        # Extract Arabic medications\n",
    "        for ar_med, en_med in self.medication_mappings.items():\n",
    "            for match in re.finditer(re.escape(ar_med), text):\n",
    "                entities.append({\n",
    "                    \"entity\": \"MEDICATION\",\n",
    "                    \"value\": ar_med,\n",
    "                    \"normalized\": en_med,\n",
    "                    \"start\": match.start(),\n",
    "                    \"end\": match.end()\n",
    "                })\n",
    "        \n",
    "        # Extract English frequencies\n",
    "        for match in self.eng_frequency_pattern.finditer(text):\n",
    "            entities.append({\n",
    "                \"entity\": \"FREQUENCY\",\n",
    "                \"value\": match.group(),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        \n",
    "        # Extract Arabic frequencies\n",
    "        for match in self.arabic_frequency_pattern.finditer(text):\n",
    "            ar_freq = match.group()\n",
    "            entities.append({\n",
    "                \"entity\": \"FREQUENCY\",\n",
    "                \"value\": ar_freq,\n",
    "                \"normalized\": self.arabic_frequencies.get(ar_freq, ar_freq),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end()\n",
    "            })\n",
    "        \n",
    "        # Sort by position in text\n",
    "        entities.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def pair_medications_with_frequencies(self, entities, text):\n",
    "        \"\"\"Pair medications with their frequencies based on proximity\"\"\"\n",
    "        medications = [e for e in entities if e[\"entity\"] == \"MEDICATION\"]\n",
    "        frequencies = [e for e in entities if e[\"entity\"] == \"FREQUENCY\"]\n",
    "        \n",
    "        pairs = []\n",
    "        \n",
    "        for med in medications:\n",
    "            # Find closest frequency\n",
    "            closest_freq = None\n",
    "            min_distance = float('inf')\n",
    "            \n",
    "            for freq in frequencies:\n",
    "                # Check if frequency follows medication\n",
    "                if freq[\"start\"] > med[\"end\"]:\n",
    "                    # Distance to next frequency\n",
    "                    distance = freq[\"start\"] - med[\"end\"]\n",
    "                    \n",
    "                    # Check if there's another medication between this med and freq\n",
    "                    has_med_between = any(\n",
    "                        m[\"start\"] > med[\"end\"] and m[\"end\"] < freq[\"start\"] \n",
    "                        for m in medications if m != med\n",
    "                    )\n",
    "                    \n",
    "                    if not has_med_between and distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        closest_freq = freq\n",
    "                        \n",
    "                        # If they're adjacent with only comma/whitespace, this is a strong match\n",
    "                        sep_text = text[med[\"end\"]:freq[\"start\"]]\n",
    "                        if re.match(r'^[\\s,]*$', sep_text):\n",
    "                            break\n",
    "            \n",
    "            # If no frequency found after, look for one before (less common but possible)\n",
    "            if closest_freq is None:\n",
    "                for freq in frequencies:\n",
    "                    if freq[\"end\"] < med[\"start\"]:\n",
    "                        distance = med[\"start\"] - freq[\"end\"]\n",
    "                        \n",
    "                        has_med_between = any(\n",
    "                            m[\"end\"] > freq[\"end\"] and m[\"start\"] < med[\"start\"] \n",
    "                            for m in medications if m != med\n",
    "                        )\n",
    "                        \n",
    "                        if not has_med_between and distance < min_distance:\n",
    "                            min_distance = distance\n",
    "                            closest_freq = freq\n",
    "            \n",
    "            # Only pair if the frequency is reasonably close\n",
    "            if closest_freq and min_distance < 50:  # Arbitrary threshold\n",
    "                pairs.append({\n",
    "                    \"medication\": med.get(\"normalized\", med[\"value\"]),\n",
    "                    \"frequency\": closest_freq.get(\"normalized\", closest_freq[\"value\"]),\n",
    "                    \"confidence\": 1.0 - (min_distance / 100)  # Higher confidence for closer pairs\n",
    "                })\n",
    "            else:\n",
    "                # Medication without clear frequency\n",
    "                pairs.append({\n",
    "                    \"medication\": med.get(\"normalized\", med[\"value\"]),\n",
    "                    \"frequency\": \"unspecified\",\n",
    "                    \"confidence\": 0.5\n",
    "                })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def process_prescription(self, text):\n",
    "        \"\"\"Process a single prescription text\"\"\"\n",
    "        # Extract entities using regex (faster for prototype)\n",
    "        entities = self.extract_entities_with_regex(text)\n",
    "        \n",
    "        # Pair medications with frequencies\n",
    "        medication_frequency_pairs = self.pair_medications_with_frequencies(entities, text)\n",
    "        \n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"entities\": entities,\n",
    "            \"medication_schedule\": medication_frequency_pairs\n",
    "        }\n",
    "    \n",
    "    def batch_process(self, prescription_list):\n",
    "        \"\"\"Process a batch of prescriptions\"\"\"\n",
    "        results = []\n",
    "        for text in prescription_list:\n",
    "            results.append(self.process_prescription(text))\n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Example prescription data\n",
    "    prescriptions = [\n",
    "        \"Carbachol Every 8 hours\",\n",
    "        \"Ofloxacin ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª, Brimonidine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Latanoprost Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\",\n",
    "        \"Brimonidine Before breakfast\",\n",
    "        \"Carbachol Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Tobramycin Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Pilocarpine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\",\n",
    "        \"Fluorometholone As needed, Latanoprost As needed\",\n",
    "        \"Brimonidine Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, Dorzolamide Ø¨Ø¹Ø¯ Ø§Ù„ØºØ¯Ø§Ø¡\",\n",
    "        \"Ø¯ÙˆØ±Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯ Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, ØªÙŠÙ…ÙˆÙ„ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Ù‚Ø·Ø±Ø§Øª Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ† ÙƒÙ„ Ù¨ Ø³Ø§Ø¹Ø§Øª, Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ† ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\",\n",
    "        \"Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ† Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹, ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\"\n",
    "    ]\n",
    "\n",
    "    # Initialize the NER system\n",
    "    ner_system = MedicalPrescriptionNER()\n",
    "\n",
    "    # Option 1: Use regex-based extraction (faster for prototyping)\n",
    "    print(\"Processing prescriptions using regex-based extraction...\")\n",
    "    results = ner_system.batch_process(prescriptions)\n",
    "\n",
    "    # Display results\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\nPrescription {i + 1}:\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(\"Medication Schedule:\")\n",
    "        for pair in result[\"medication_schedule\"]:\n",
    "            print(f\"  - Medication: {pair['medication']}, Frequency: {pair['frequency']} (Confidence: {pair['confidence']:.2f})\")\n",
    "\n",
    "    # Option 2: Load a pre-trained model (if available)\n",
    "    # Uncomment the following lines to use a pre-trained model instead of regex:\n",
    "    # print(\"\\nLoading pre-trained model...\")\n",
    "    # ner_system.load_pretrained_model()\n",
    "    # results = ner_system.batch_process(prescriptions)\n",
    "\n",
    "    # Option 3: Train a custom model (if labeled data is available)\n",
    "    # Uncomment the following lines to prepare training data and train a model:\n",
    "    # print(\"\\nPreparing training data...\")\n",
    "    # training_data = ner_system.prepare_training_data(prescriptions)\n",
    "    # print(\"\\nTraining spaCy model...\")\n",
    "    # ner_system.train_spacy_model(training_data)\n",
    "    # print(\"\\nTraining transformer model...\")\n",
    "    # ner_system.train_transformer_model(training_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Define the NER system class\n",
    "class MedicalPrescriptionNER:\n",
    "    def __init__(self):\n",
    "        self.medication_list = [\"Carbachol\", \"Ofloxacin\", \"Brimonidine\", \"Latanoprost\", \"Pilocarpine\", \"Tobramycin\", \"Fluorometholone\", \"Dorzolamide\", \"Timolol\", \"Prednisolone\"]\n",
    "        self.frequency_patterns = [\n",
    "            (r'ÙƒÙ„\\s*(\\d+)\\s*Ø³Ø§Ø¹Ø§Øª', 'every \\1 hours'),\n",
    "            (r'Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…', 'before bed'),\n",
    "            (r'Ø¨Ø¹Ø¯ Ø§Ù„ØºØ¯Ø§Ø¡', 'after lunch'),\n",
    "            (r'Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹', 'once daily'),\n",
    "            (r'Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…', 'as needed'),\n",
    "            (r'Every (\\d+) hours', 'every \\1 hours'),\n",
    "            (r'Before breakfast', 'before breakfast'),\n",
    "            (r'As needed', 'as needed')\n",
    "        ]\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        entities = []\n",
    "        for med in self.medication_list:\n",
    "            if med in text:\n",
    "                entities.append((med, 'MEDICATION'))\n",
    "        for pattern, freq in self.frequency_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    extracted_freq = freq.replace('\\\\1', match) if '\\\\1' in freq else freq\n",
    "                    entities.append((extracted_freq, 'FREQUENCY'))\n",
    "        return entities\n",
    "    \n",
    "    def batch_process(self, prescriptions):\n",
    "        processed_data = []\n",
    "        for text in prescriptions:\n",
    "            entities = self.extract_entities(text)\n",
    "            processed_data.append({\"text\": text, \"entities\": entities})\n",
    "        return processed_data\n",
    "\n",
    "def prepare_training_data(processed_data):\n",
    "    labeled_data = []\n",
    "    for entry in processed_data:\n",
    "        text = entry['text']\n",
    "        labels = ['O'] * len(text)\n",
    "        for entity, label in entry['entities']:\n",
    "            start_idx = text.find(entity)\n",
    "            if start_idx != -1:\n",
    "                for i in range(start_idx, start_idx + len(entity)):\n",
    "                    labels[i] = 'B-' + label if i == start_idx else 'I-' + label\n",
    "        labeled_data.append({\"tokens\": list(text), \"labels\": labels})\n",
    "    return labeled_data\n",
    "\n",
    "def train_ner_model(training_data):\n",
    "    model_name = \"distilbert-base-multilingual-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=3)\n",
    "    \n",
    "    tokenized_inputs = tokenizer([d['tokens'] for d in training_data], truncation=True, padding=True, is_split_into_words=True)\n",
    "    labels = [[0 if l == 'O' else 1 for l in d['labels']] for d in training_data]\n",
    "    \n",
    "    dataset = [{\"input_ids\": tokenized_inputs['input_ids'][i], \"attention_mask\": tokenized_inputs['attention_mask'][i], \"labels\": labels[i]} for i in range(len(labels))]\n",
    "    \n",
    "    training_args = TrainingArguments(output_dir=\"./ner_model\", num_train_epochs=3, per_device_train_batch_size=4)\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=dataset)\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./ner_model\")\n",
    "\n",
    "def main():\n",
    "    prescriptions = [\n",
    "        \"Carbachol Every 8 hours\",\n",
    "        \"Ofloxacin ÙƒÙ„ Ø³Øª Ø³Ø§Ø¹Ø§Øª, Brimonidine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Latanoprost Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\",\n",
    "        \"Brimonidine Before breakfast\",\n",
    "        \"Carbachol Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Tobramycin Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Pilocarpine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…\",\n",
    "        \"Fluorometholone As needed, Latanoprost As needed\",\n",
    "        \"Brimonidine Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, Dorzolamide Ø¨Ø¹Ø¯ Ø§Ù„ØºØ¯Ø§Ø¡\",\n",
    "        \"Ø¯ÙˆØ±Ø²ÙˆÙ„Ø§Ù…ÙŠØ¯ Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, ØªÙŠÙ…ÙˆÙ„ÙˆÙ„ Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Ù‚Ø·Ø±Ø§Øª Ø¨Ø±ÙŠØ¯Ù†ÙŠØ²ÙˆÙ„ÙˆÙ† ÙƒÙ„ Ù¨ Ø³Ø§Ø¹Ø§Øª, Ø£ÙˆÙÙ„ÙˆÙƒØ³Ø§Ø³ÙŠÙ† ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\",\n",
    "        \"Ù„Ø§ØªØ§Ù†ÙˆØ¨Ø±ÙˆØ³Øª Ù‚Ø¨Ù„ Ø§Ù„Ù†ÙˆÙ…, ØªÙˆØ¨Ø±Ø§Ù…ÙŠØ³ÙŠÙ† Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹, ÙƒØ§Ø±Ø¨Ø§Ø´ÙˆÙ„ ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª\"\n",
    "    ]\n",
    "\n",
    "    ner_system = MedicalPrescriptionNER()\n",
    "    results = ner_system.batch_process(prescriptions)\n",
    "    training_data = prepare_training_data(results)\n",
    "    \n",
    "    with open(\"training_data.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(training_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(\"Training data saved! Starting model training...\")\n",
    "    train_ner_model(training_data)\n",
    "    print(\"Model training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_name = \"med_ner_model\"  # Change this to your actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Initialize a NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_medications(text):\n",
    "    ner_results = ner_pipeline(text)\n",
    "    \n",
    "    extracted_entities = []\n",
    "    current_entity = {\"word\": \"\", \"entity\": \"\"}\n",
    "    \n",
    "    for entity in ner_results:\n",
    "        word = entity[\"word\"].replace(\"â–\", \"\")  # Handling subword tokenization artifacts\n",
    "        if entity[\"entity\"].startswith(\"B-\"):  # Beginning of a new entity\n",
    "            if current_entity[\"word\"]:  # Save the previous entity\n",
    "                extracted_entities.append(current_entity)\n",
    "            current_entity = {\"word\": word, \"entity\": entity[\"entity\"][2:]}\n",
    "        elif entity[\"entity\"].startswith(\"I-\") and current_entity[\"entity\"]:  # Inside the same entity\n",
    "            current_entity[\"word\"] += \" \" + word\n",
    "    \n",
    "    # Add the last entity\n",
    "    if current_entity[\"word\"]:\n",
    "        extracted_entities.append(current_entity)\n",
    "\n",
    "    return extracted_entities\n",
    "\n",
    "# Example prediction\n",
    "prescription_text = \"Brimonidine ÙƒÙ„ Ù¦ Ø³Ø§Ø¹Ø§Øª, Carbachol Ø¹Ù†Ø¯ Ø§Ù„Ù„Ø²ÙˆÙ…, Latanoprost Ù…Ø±Ø© ÙŠÙˆÙ…ÙŠØ§Ù‹\"\n",
    "predictions = predict_medications(prescription_text)\n",
    "\n",
    "# Display results\n",
    "for entity in predictions:\n",
    "    print(f\"Extracted: {entity['word']} - Label: {entity['entity']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
